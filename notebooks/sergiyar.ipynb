{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9372a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c7697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading records: 9999it [00:06, 1539.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from habr_article_analyzer.data import download_dataset, load_dataset_from_zst\n",
    "\n",
    "download_dataset()\n",
    "dataset = load_dataset_from_zst(rows_num=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9feba9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               10000 non-null  int64 \n",
      " 1   language         10000 non-null  object\n",
      " 2   url              10000 non-null  object\n",
      " 3   text_markdown    10000 non-null  object\n",
      " 4   text_html        10000 non-null  object\n",
      " 5   lead_markdown    9993 non-null   object\n",
      " 6   lead_html        10000 non-null  object\n",
      " 7   type             10000 non-null  object\n",
      " 8   labels           10000 non-null  object\n",
      " 9   original_author  973 non-null    object\n",
      " 10  original_url     973 non-null    object\n",
      " 11  time_published   10000 non-null  int64 \n",
      " 12  author           10000 non-null  object\n",
      " 13  title            10000 non-null  object\n",
      " 14  statistics       10000 non-null  object\n",
      " 15  hubs             10000 non-null  object\n",
      " 16  flows            10000 non-null  object\n",
      " 17  tags             10000 non-null  object\n",
      " 18  reading_time     10000 non-null  int64 \n",
      " 19  format           550 non-null    object\n",
      " 20  complexity       10 non-null     object\n",
      " 21  comments         10000 non-null  object\n",
      "dtypes: int64(3), object(19)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670edbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habr_article_analyzer.targets import Target\n",
    "\n",
    "hubs = Target(dataset, \"hubs\")\n",
    "flows = Target(dataset, \"flows\")\n",
    "tags = Target(dataset, \"tags\", sparse=True)\n",
    "labels = Target(dataset, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbabd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_len(word: str, n=10):\n",
    "    if len(word) >= n + 3:\n",
    "        return word[:n] + \"...\"\n",
    "    else:\n",
    "        return word + \" \" * (n - (len(word)))\n",
    "\n",
    "\n",
    "def info(self: Target, top=20):\n",
    "    print(f\"Target: {self.column_name}\")\n",
    "    print(f\"There are {len(self.labels)} unique labels\")\n",
    "    sorted_by_size = [\n",
    "        (label, size)\n",
    "        for label, size in sorted(\n",
    "            zip(self.labels, self.get_sizes()), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    def print_list(sorted_group):\n",
    "        for i, (label, size) in enumerate(sorted_group):\n",
    "            coverage = self.get_coverage(label)\n",
    "            print(f\"{i}:\\t{fix_len(label)}\\t{size}\\t{coverage*100:.2f}%\")\n",
    "\n",
    "    if len(self.labels) > top * 2:\n",
    "        print(f\"Top {top} labels by their size:\")\n",
    "        print_list(sorted_by_size[:top])\n",
    "        print(f\"Tail {top} labels by their size:\")\n",
    "        print_list(sorted_by_size[-top:])\n",
    "    else:\n",
    "        print(f\"Labels by their size:\")\n",
    "        print_list(sorted_by_size)\n",
    "\n",
    "    print(f\"Sum coverage by top-N labels:\")\n",
    "    percent_to_cover = [50, 75, 80, 90, 95, 99]\n",
    "\n",
    "    for percent in percent_to_cover:\n",
    "        l, r = 0, len(self)\n",
    "        while l < r:\n",
    "            m = (l + r) // 2\n",
    "            coverage = self.get_coverage([label for label, _ in sorted_by_size[:m]])\n",
    "            if coverage * 100 <= percent:\n",
    "                l = m + 1\n",
    "            else:\n",
    "                r = m\n",
    "        print(f\"Top-{m} labels cover ~{percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5c6335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: hubs\n",
      "There are 1127 unique labels\n",
      "Top 20 labels by their size:\n",
      "0:\tcloset    \t900\t9.00%\n",
      "1:\titcompanies\t690\t6.90%\n",
      "2:\tinfosecurity\t617\t6.17%\n",
      "3:\tprogramming\t548\t5.48%\n",
      "4:\twebdev    \t494\t4.94%\n",
      "5:\tpopular_sc...\t463\t4.63%\n",
      "6:\tgadgets   \t302\t3.02%\n",
      "7:\tjavascript\t302\t3.02%\n",
      "8:\tfinance   \t249\t2.49%\n",
      "9:\tit-infrast...\t239\t2.39%\n",
      "10:\tcareer    \t238\t2.38%\n",
      "11:\thardware  \t234\t2.34%\n",
      "12:\tpython    \t227\t2.27%\n",
      "13:\tbusiness-l...\t224\t2.24%\n",
      "14:\topen_source\t215\t2.15%\n",
      "15:\tgamedev   \t213\t2.13%\n",
      "16:\tsocial_net...\t211\t2.11%\n",
      "17:\tDIY       \t210\t2.10%\n",
      "18:\tandroid_dev\t210\t2.10%\n",
      "19:\tmachine_le...\t210\t2.10%\n",
      "Tail 20 labels by their size:\n",
      "0:\tvezetvsem \t1\t0.01%\n",
      "1:\tvideointel...\t1\t0.01%\n",
      "2:\tvirt2real \t1\t0.01%\n",
      "3:\tvisiology \t1\t0.01%\n",
      "4:\tvivid_money\t1\t0.01%\n",
      "5:\tvps_house \t1\t0.01%\n",
      "6:\tvrdevice  \t1\t0.01%\n",
      "7:\tvsk_insura...\t1\t0.01%\n",
      "8:\twayforpay \t1\t0.01%\n",
      "9:\twayray    \t1\t0.01%\n",
      "10:\twebo      \t1\t0.01%\n",
      "11:\twestcomp  \t1\t0.01%\n",
      "12:\twhat3words\t1\t0.01%\n",
      "13:\tximad     \t1\t0.01%\n",
      "14:\txslt      \t1\t0.01%\n",
      "15:\tyadro     \t1\t0.01%\n",
      "16:\tyouvend   \t1\t0.01%\n",
      "17:\tzerotech  \t1\t0.01%\n",
      "18:\tzte       \t1\t0.01%\n",
      "19:\tzwave     \t1\t0.01%\n",
      "Sum coverage by top-N labels:\n",
      "Top-16 labels cover ~50%\n",
      "Top-50 labels cover ~75%\n",
      "Top-74 labels cover ~80%\n",
      "Top-164 labels cover ~90%\n",
      "Top-307 labels cover ~95%\n",
      "Top-766 labels cover ~99%\n"
     ]
    }
   ],
   "source": [
    "info(hubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2980371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: flows\n",
      "There are 6 unique labels\n",
      "Labels by their size:\n",
      "0:\tdevelop   \t4417\t44.17%\n",
      "1:\tpopsci    \t4103\t41.03%\n",
      "2:\tmanagement\t1626\t16.26%\n",
      "3:\tadmin     \t1034\t10.34%\n",
      "4:\tmarketing \t541\t5.41%\n",
      "5:\tdesign    \t388\t3.88%\n",
      "Sum coverage by top-N labels:\n",
      "Top-2 labels cover ~50%\n",
      "Top-2 labels cover ~75%\n",
      "Top-2 labels cover ~80%\n",
      "Top-4 labels cover ~90%\n",
      "Top-5 labels cover ~95%\n",
      "Top-5 labels cover ~99%\n"
     ]
    }
   ],
   "source": [
    "info(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617960d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: tags\n",
      "There are 23259 unique labels\n",
      "Top 20 labels by their size:\n",
      "0:\tjavascript\t177\t1.77%\n",
      "1:\tgoogle    \t166\t1.66%\n",
      "2:\tandroid   \t165\t1.65%\n",
      "3:\tpython    \t161\t1.61%\n",
      "4:\tlinux     \t145\t1.45%\n",
      "5:\tразработка\t141\t1.41%\n",
      "6:\tjava      \t136\t1.36%\n",
      "7:\tapple     \t122\t1.22%\n",
      "8:\tphp       \t122\t1.22%\n",
      "9:\tmicrosoft \t121\t1.21%\n",
      "10:\tпрограммир...\t121\t1.21%\n",
      "11:\tинформацио...\t109\t1.09%\n",
      "12:\tGoogle    \t100\t1.00%\n",
      "13:\tигры      \t96\t0.96%\n",
      "14:\tмашинное о...\t95\t0.95%\n",
      "15:\topen source\t85\t0.85%\n",
      "16:\tстартапы  \t84\t0.84%\n",
      "17:\tбезопасность\t79\t0.79%\n",
      "18:\tдизайн    \t78\t0.78%\n",
      "19:\tios       \t75\t0.75%\n",
      "Tail 20 labels by their size:\n",
      "0:\tяндекс.бра...\t1\t0.01%\n",
      "1:\tяндекс.диск\t1\t0.01%\n",
      "2:\tяндекс.драйв\t1\t0.01%\n",
      "3:\tяндекс.кал...\t1\t0.01%\n",
      "4:\tяндекс.лок...\t1\t0.01%\n",
      "5:\tяндекс.мет...\t1\t0.01%\n",
      "6:\tяндекс.метро\t1\t0.01%\n",
      "7:\tяндекс.нав...\t1\t0.01%\n",
      "8:\tяндекс.обл...\t1\t0.01%\n",
      "9:\tяндекс.под...\t1\t0.01%\n",
      "10:\tяндекс.сайт\t1\t0.01%\n",
      "11:\tяндекс.спр...\t1\t0.01%\n",
      "12:\tянки      \t1\t0.01%\n",
      "13:\tяпонские у...\t1\t0.01%\n",
      "14:\tяпонские у...\t1\t0.01%\n",
      "15:\tяпонский д...\t1\t0.01%\n",
      "16:\tярлыки    \t1\t0.01%\n",
      "17:\tярость    \t1\t0.01%\n",
      "18:\tячейки    \t1\t0.01%\n",
      "19:\tёлочные ги...\t1\t0.01%\n",
      "Sum coverage by top-N labels:\n",
      "Top-163 labels cover ~50%\n",
      "Top-824 labels cover ~75%\n",
      "Top-1294 labels cover ~80%\n",
      "Top-4173 labels cover ~90%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36minfo\u001b[39m\u001b[34m(self, top)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m l < r:\n\u001b[32m     35\u001b[39m     m = (l + r) // \u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     coverage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msorted_by_size\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coverage * \u001b[32m100\u001b[39m <= percent:\n\u001b[32m     38\u001b[39m         l = m + \u001b[32m1\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:74\u001b[39m, in \u001b[36mget_coverage\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     70\u001b[39m     idx = \u001b[38;5;28mself\u001b[39m.labels\n\u001b[32m     72\u001b[39m binary_submask = \u001b[38;5;28mself\u001b[39m[idx]\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(binary_submask):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary_submask.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (binary_submask.getnnz() / binary_submask.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:57\u001b[39m, in \u001b[36mTarget.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mself\u001b[39m.label_to_id(idx)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, abc.Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     idx = np.array(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(\u001b[38;5;28mself\u001b[39m.binary_mask):\n\u001b[32m     60\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.binary_mask[:, idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:57\u001b[39m, in \u001b[36mTarget.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mself\u001b[39m.label_to_id(idx)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, abc.Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     idx = np.array(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x, idx)))\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(\u001b[38;5;28mself\u001b[39m.binary_mask):\n\u001b[32m     60\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.binary_mask[:, idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:47\u001b[39m, in \u001b[36mTarget.label_to_id\u001b[39m\u001b[34m(self, label)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not in labels\u001b[39m\u001b[33m\"\u001b[39m.format(label))\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "info(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d441958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: labels\n",
      "There are 8 unique labels\n",
      "Labels by their size:\n",
      "0:\ttranslation\t973\t9.73%\n",
      "1:\tsandbox   \t808\t8.08%\n",
      "2:\trecovery  \t212\t2.12%\n",
      "3:\ttechnotext...\t20\t0.20%\n",
      "4:\ttechnotext...\t20\t0.20%\n",
      "5:\ttechnotext...\t19\t0.19%\n",
      "6:\tseasonJava...\t2\t0.02%\n",
      "7:\tseasonDm2022\t1\t0.01%\n",
      "Sum coverage by top-N labels:\n",
      "Top-7 labels cover ~50%\n",
      "Top-7 labels cover ~75%\n",
      "Top-7 labels cover ~80%\n",
      "Top-7 labels cover ~90%\n",
      "Top-7 labels cover ~95%\n",
      "Top-7 labels cover ~99%\n"
     ]
    }
   ],
   "source": [
    "info(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d7659",
   "metadata": {},
   "source": [
    "### Выбор типа метки\n",
    "\n",
    "Выше приведены сводки информации по каждому из четырех видов текстовых меток: `['hubs', 'tags', 'labels', 'flows']`. \n",
    "Видно, что `['hubs', 'tags']` - метки с большим количеством классов относящихся к содержанию текста, а `['labels', 'flows']` - скорее технические метки с маленьким количеством классов. Хотя `flows` отражает некоторую категорию текста, эта категория слишком абстрактна (sci-fi, develop) для нашей задачи.\n",
    "\n",
    "Поэтому, в контексте нашей задачи, интересны именно метки `hubs` и `tags`. \n",
    "Видно, что для покрытия более 95% примеров в датасете, нужно будет использовать примерно 200 и 2000 классов из каждого типа меток. \n",
    "Это звучит как слишком большое число меток, при этом не достигается полного покрытия. \n",
    "\n",
    "Далее посмотрим на примеры, которые не покрыты этим множествами меток, а так же рассмотрим рандомные примеры из каждого из этих множеств."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb818d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_sample(self: Target, n_samples: int = 20):\n",
    "    print(f\"Random samples for '{self.column_name}':\")\n",
    "    print(\"\\n\".join(random.sample(self.labels, n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb25ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples for 'hubs':\n",
      "cyberleninka\n",
      "akbarsdigital\n",
      "kokoc_group\n",
      "intersystems\n",
      "htmlacademy\n",
      "drupal\n",
      "cpp\n",
      "yota\n",
      "cloudsnn\n",
      "saas\n",
      "analogbytes\n",
      "allcorrect\n",
      "comet-server\n",
      "host-tracker\n",
      "igromagaz\n",
      "uprock\n",
      "alawar\n",
      "google_chrome\n",
      "rusonyx\n",
      "hpe\n"
     ]
    }
   ],
   "source": [
    "labels_sample(hubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38856136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random samples for 'tags':\n",
      "гост р 34.10-2001\n",
      "предсказательная аналитика\n",
      "prince2\n",
      "3d печать\n",
      "curl\n",
      "широкополосный интернет\n",
      "большие данные\n",
      "vm/sp cms rexx\n",
      "premium account\n",
      "convolutional neural network\n",
      "inkscape; vector graphics\n",
      "vector magic\n",
      "TechCrunch\n",
      "SP Manager Lite 5 бесплатно\n",
      "ПСПО\n",
      "Smart TV\n",
      "БД\n",
      "web design\n",
      "работа со списками\n",
      "opera mini 4\n"
     ]
    }
   ],
   "source": [
    "labels_sample(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f1208",
   "metadata": {},
   "source": [
    "Видно, что среди тегов встречаются как на русском языке, так и на английском. Проверим есть ли дубли на нескольких примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c93578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pair: ['deep learning', 'нейронные сети']\n",
      "deep learn...\t0.18\n",
      "нейронные ...\t0.27\n",
      "Both      \t0.42\n",
      "For pair: ['нейросети', 'нейронные сети']\n",
      "нейросети \t0.33999999999999997\n",
      "нейронные ...\t0.27\n",
      "Both      \t0.5700000000000001\n",
      "For pair: ['deep learning', 'глубокое обучение']\n",
      "deep learn...\t0.18\n",
      "глубокое о...\t0.1\n",
      "Both      \t0.24\n",
      "Got an exception: 'develop not in labels'\n",
      "For pair: ['network', 'сеть']\n",
      "network   \t0.05\n",
      "сеть      \t0.16999999999999998\n",
      "Both      \t0.22\n",
      "For pair: ['AI', 'ИИ']\n",
      "AI        \t0.06999999999999999\n",
      "ИИ        \t0.13999999999999999\n",
      "Both      \t0.2\n",
      "For pair: ['machine learning', 'машинное обучение']\n",
      "machine le...\t0.44\n",
      "машинное о...\t0.95\n",
      "Both      \t1.2\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    [\"deep learning\", \"нейронные сети\"],\n",
    "    [\"нейросети\", \"нейронные сети\"],\n",
    "    [\"deep learning\", \"глубокое обучение\"],\n",
    "    [\"develop\", \"разработка\"],\n",
    "    [\"network\", \"сеть\"],\n",
    "    [\"AI\", \"ИИ\"],\n",
    "    [\"machine learning\", \"машинное обучение\"],\n",
    "]\n",
    "\n",
    "for pair in pairs:\n",
    "    en, ru = pair\n",
    "    try:\n",
    "        en_cov = tags.get_coverage(en) * 100\n",
    "        ru_cov = tags.get_coverage(ru) * 100\n",
    "        both_cov = tags.get_coverage(pair) * 100\n",
    "        print(f\"For pair: {pair}\")\n",
    "        print(f\"{fix_len(en)}\\t{en_cov}\")\n",
    "        print(f\"{fix_len(ru)}\\t{ru_cov}\")\n",
    "        print(f\"{fix_len(\"Both\")}\\t{both_cov}\")\n",
    "    except Exception as e:\n",
    "        print(\"Got an exception: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fb4f1",
   "metadata": {},
   "source": [
    "Видно, что теги не структурированы - встречаются дубли как на одном (`нейросети` - `нейронные сети`), так и на разных языках (`machine learning` - `машинное обучение`). В таком виде эти данные не очень хорошо подходят для обучения модели, так как эти классы не будут отличаться по смыслу, при этом будут разными классами, что усложняет обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae00c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_labels_for_uncovered_samples(\n",
    "    self: Target = hubs, top_n_selected=200, n_sample=20\n",
    "):\n",
    "    sorted_by_size = [\n",
    "        label\n",
    "        for label, _ in sorted(\n",
    "            zip(self.labels, self.get_sizes()), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    uncovered = np.logical_not(self[sorted_by_size[:top_n_selected]].any(axis=1))\n",
    "    coverages_count = self[self.labels][uncovered].sum(axis=0)\n",
    "    n_best = [self.labels[i] for i in np.argsort(coverages_count)][:20]\n",
    "\n",
    "    print(f\"Target: {self.column_name}\")\n",
    "\n",
    "    for i, label in enumerate(n_best):\n",
    "        size = self.get_sizes(label)\n",
    "        coverage = self.get_coverage(label)\n",
    "        print(f\"{i}:\\t{fix_len(label, 16)}\\t{size}\\t{coverage*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab95cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: hubs\n",
      "0:\tmr_gefest       \t1\t0.01%\n",
      "1:\tmobile_dimension\t1\t0.01%\n",
      "2:\tmobile_one      \t2\t0.02%\n",
      "3:\tmobile_testing  \t26\t0.26%\n",
      "4:\tmobileanalytics \t23\t0.23%\n",
      "5:\tmobileup        \t3\t0.03%\n",
      "6:\tmobio           \t5\t0.05%\n",
      "7:\tmodesco         \t1\t0.01%\n",
      "8:\tmoex            \t2\t0.02%\n",
      "9:\tmomondo         \t1\t0.01%\n",
      "10:\tmonandco        \t1\t0.01%\n",
      "11:\tmono            \t1\t0.01%\n",
      "12:\tmoonmodule      \t1\t0.01%\n",
      "13:\tmoysklad        \t1\t0.01%\n",
      "14:\tmobile_dev      \t210\t2.10%\n",
      "15:\tmygames         \t2\t0.02%\n",
      "16:\tnag             \t2\t0.02%\n",
      "17:\tnatural_language...\t26\t0.26%\n",
      "18:\tnavicon         \t1\t0.01%\n",
      "19:\tncloudtech      \t8\t0.08%\n"
     ]
    }
   ],
   "source": [
    "get_best_labels_for_uncovered_samples(self=hubs, top_n_selected=200, n_sample=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c9eb4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_best_labels_for_uncovered_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n_selected\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_best_labels_for_uncovered_samples\u001b[39m\u001b[34m(self, top_n_selected, n_sample)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_best_labels_for_uncovered_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m: Target=hubs, top_n_selected=\u001b[32m200\u001b[39m, n_sample=\u001b[32m20\u001b[39m):\n\u001b[32m      2\u001b[39m     sorted_by_size = [\n\u001b[32m      3\u001b[39m         label \u001b[38;5;28;01mfor\u001b[39;00m label, _ \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.labels, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sizes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m      7\u001b[39m     uncovered = np.logical_not(\u001b[38;5;28mself\u001b[39m[sorted_by_size[:top_n_selected]].any(axis=\u001b[32m1\u001b[39m))\n\u001b[32m      8\u001b[39m     coverages_count = \u001b[38;5;28mself\u001b[39m[\u001b[38;5;28mself\u001b[39m.labels][uncovered].sum(axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:97\u001b[39m, in \u001b[36mget_sizes\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     95\u001b[39m         return int(result[0]) if result.size == 1 else int(result)\n\u001b[32m     96\u001b[39m     return result\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m else:\n\u001b[32m     98\u001b[39m     result = binary_submask.sum(axis=0)\n\u001b[32m     99\u001b[39m     if np.isscalar(result) or (hasattr(result, 'size') and result.size == 1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:57\u001b[39m, in \u001b[36mTarget.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mself\u001b[39m.label_to_id(idx)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, abc.Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     idx = np.array(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(\u001b[38;5;28mself\u001b[39m.binary_mask):\n\u001b[32m     60\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.binary_mask[:, idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:57\u001b[39m, in \u001b[36mTarget.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mself\u001b[39m.label_to_id(idx)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, abc.Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     idx = np.array(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x, idx)))\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sp.issparse(\u001b[38;5;28mself\u001b[39m.binary_mask):\n\u001b[32m     60\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.binary_mask[:, idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/src/habr_article_analyzer/targets.py:47\u001b[39m, in \u001b[36mTarget.label_to_id\u001b[39m\u001b[34m(self, label)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not in labels\u001b[39m\u001b[33m\"\u001b[39m.format(label))\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "get_best_labels_for_uncovered_samples(self=tags, top_n_selected=2000, n_sample=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab9380",
   "metadata": {},
   "source": [
    "В `tags` встречаются довольно разнообразные лейблы. Я попробовал написать статью (тестовую) на habr.com для того, чтобы разобраться, как проставляются `hubs` (хабы) и `tags` (ключевые слова или теги). \n",
    "\n",
    "Теги можно выбрать из выпадающего списка, а можно - добавить свой новый тег. \n",
    "Хабы же можно выбирать из большого, но фиксированного списка.\n",
    "\n",
    "В таком случае для обучения классификатора лучше подходят хабы. Но при этом их все ещё слишком много. \n",
    "\n",
    "Одним из решений в таком случае было бы оставить топ-100/топ-200 хабов, обеспечивающие наибольшее покрытие статей. \n",
    "Но я хочу рассмаотреть альтернативный способ, позволяющий не жертвовать покрытием совсем - кластеризовать все хабы на N кластеров и выбрать эти группы в качестве таргетов для модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e93e3",
   "metadata": {},
   "source": [
    "В данном случае у нас нет готовых (или зафиксированных) эмбеддингов для кластеризации, что не позволяет нам использовать большинство метрик кластеризации для сравнения качества полученных групп лейблов. \n",
    "\n",
    "Поэтому я предлагаю сделать довольно неприятную, зато честную работу: \n",
    "- разметить 40 пар лейблов бинарно: должны они попадать в один класс или нет\n",
    "- домайнить позитивов в эту разметку, посредством тщательного анализа лейблов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0c23d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"asp\",\n",
      "        \"jquery\"\n",
      "    ],\n",
      "    [\n",
      "        \"speechpro\",\n",
      "        \"brainfuck\"\n",
      "    ],\n",
      "    [\n",
      "        \"wolfram\",\n",
      "        \"haulmont\"\n",
      "    ],\n",
      "    [\n",
      "        \"design\",\n",
      "        \"mneniya_pro\"\n",
      "    ],\n",
      "    [\n",
      "        \"pochtoy\",\n",
      "        \"health\"\n",
      "    ],\n",
      "    [\n",
      "        \"cgi\",\n",
      "        \"ulmart\"\n",
      "    ],\n",
      "    [\n",
      "        \"localization\",\n",
      "        \"quarkly\"\n",
      "    ],\n",
      "    [\n",
      "        \"otkritie_broker\",\n",
      "        \"speakasap\"\n",
      "    ],\n",
      "    [\n",
      "        \"getmatch\",\n",
      "        \"e-legion\"\n",
      "    ],\n",
      "    [\n",
      "        \"buruki\",\n",
      "        \"javascript\"\n",
      "    ],\n",
      "    [\n",
      "        \"astronomy\",\n",
      "        \"drupal\"\n",
      "    ],\n",
      "    [\n",
      "        \"icanchoose\",\n",
      "        \"ios_dev\"\n",
      "    ],\n",
      "    [\n",
      "        \"lifehacks\",\n",
      "        \"x-com\"\n",
      "    ],\n",
      "    [\n",
      "        \"cian\",\n",
      "        \"ivideon\"\n",
      "    ],\n",
      "    [\n",
      "        \"igromagaz\",\n",
      "        \"sciberia\"\n",
      "    ],\n",
      "    [\n",
      "        \"courson\",\n",
      "        \"rambler\"\n",
      "    ],\n",
      "    [\n",
      "        \"popular_science\",\n",
      "        \"otus\"\n",
      "    ],\n",
      "    [\n",
      "        \"kryptonite\",\n",
      "        \"hosting-cafe\"\n",
      "    ],\n",
      "    [\n",
      "        \"softmart\",\n",
      "        \"droider\"\n",
      "    ],\n",
      "    [\n",
      "        \"panda\",\n",
      "        \"vdsina\"\n",
      "    ],\n",
      "    [\n",
      "        \"farminers\",\n",
      "        \"tuturu\"\n",
      "    ],\n",
      "    [\n",
      "        \"pechkin\",\n",
      "        \"codefest\"\n",
      "    ],\n",
      "    [\n",
      "        \"oop\",\n",
      "        \"atlasbiomed\"\n",
      "    ],\n",
      "    [\n",
      "        \"hardware\",\n",
      "        \"cubicrobotics\"\n",
      "    ],\n",
      "    [\n",
      "        \"joom\",\n",
      "        \"energy\"\n",
      "    ],\n",
      "    [\n",
      "        \"c\",\n",
      "        \"evrone\"\n",
      "    ],\n",
      "    [\n",
      "        \"flipperdevices\",\n",
      "        \"saas\"\n",
      "    ],\n",
      "    [\n",
      "        \"what3words\",\n",
      "        \"bitrix\"\n",
      "    ],\n",
      "    [\n",
      "        \"intersystems\",\n",
      "        \"presentation\"\n",
      "    ],\n",
      "    [\n",
      "        \"ffcms\",\n",
      "        \"posttech\"\n",
      "    ],\n",
      "    [\n",
      "        \"sqalab\",\n",
      "        \"study\"\n",
      "    ],\n",
      "    [\n",
      "        \"polyglot\",\n",
      "        \"web_payment_ru\"\n",
      "    ],\n",
      "    [\n",
      "        \"kabanchik\",\n",
      "        \"ispmanager\"\n",
      "    ],\n",
      "    [\n",
      "        \"hosting-cafe\",\n",
      "        \"IPFS\"\n",
      "    ],\n",
      "    [\n",
      "        \"mosigra\",\n",
      "        \"dart\"\n",
      "    ],\n",
      "    [\n",
      "        \"banki\",\n",
      "        \"wordpress\"\n",
      "    ],\n",
      "    [\n",
      "        \"iconoskaz\",\n",
      "        \"joom\"\n",
      "    ],\n",
      "    [\n",
      "        \"analogbytes\",\n",
      "        \"netwrix\"\n",
      "    ],\n",
      "    [\n",
      "        \"mobile_dev\",\n",
      "        \"cyberleninka\"\n",
      "    ],\n",
      "    [\n",
      "        \"rdp\",\n",
      "        \"lodoss\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample1, sample2 = random.sample(hubs.labels, 40), random.sample(hubs.labels, 40)\n",
    "\n",
    "print(json.dumps(list(zip(sample1, sample2)), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce2596",
   "metadata": {},
   "source": [
    "```python\n",
    "[\n",
    "    [\n",
    "        \"microformats\",\n",
    "        \"edge\",\n",
    "        0\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        \"masterkit\",\n",
    "        \"medgadgets\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"asus\",\n",
    "        \"social_networks\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"skyeng\",\n",
    "        \"cpu\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"alconost\",\n",
    "        \"metrotek\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"vertdider\",\n",
    "        \"wunderfund\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"biology\",\n",
    "        \"beeline\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"cloudsnn\",\n",
    "        \"virtualization\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"plarium\",\n",
    "        \"pixonic\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"compilers\",\n",
    "        \"gadgets\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"study\",\n",
    "        \"unisender\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"refactoring\",\n",
    "        \"postgresql\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"itcompanies\",\n",
    "        \"pay_system\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"owasp\",\n",
    "        \"cybersport\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"antikvariat\",\n",
    "        \"cloudsnn\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"image_processing\",\n",
    "        \"html5\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"ie\",\n",
    "        \"complete_code\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"learning_languages\",\n",
    "        \"naumen\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"mongodb\",\n",
    "        \"sport_programming\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"e-legion\",\n",
    "        \"yandex_api\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"network_standarts\",\n",
    "        \"mssql\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"3d_graphics\",\n",
    "        \"xml\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"sberdevices\",\n",
    "        \"1C\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"bitrix\",\n",
    "        \"ozontech\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"regex\",\n",
    "        \"getwear\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"vtb\",\n",
    "        \"macloud\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"nano\",\n",
    "        \"AJAX\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"skillbox\",\n",
    "        \"biotech\"\n",
    "    ],\n",
    "    [\n",
    "        \"energy\",\n",
    "        \"youvend\"\n",
    "    ],\n",
    "    [\n",
    "        \"sound\",\n",
    "        \"hi\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"powershell\",\n",
    "        \"vivaldi\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"growthhacking\",\n",
    "        \"typography\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"biotech\",\n",
    "        \"tablum\"\n",
    "    ],\n",
    "    [\n",
    "        \"funcprog\",\n",
    "        \"game_testing\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"ivideon\",\n",
    "        \"doctrine\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"community_management\",\n",
    "        \"analysis_design\",\n",
    "        0.5\n",
    "    ],\n",
    "    [\n",
    "        \"kebrum\",\n",
    "        \"domclick\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"1cloud\",\n",
    "        \"notebooks\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"visual_programming\",\n",
    "        \"arttel\",\n",
    "        0\n",
    "    ],\n",
    "    [\n",
    "        \"sberbank\",\n",
    "        \"ashmanov_net\",\n",
    "        0\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8a72f",
   "metadata": {},
   "source": [
    "После такой небольшой разметки, хочется выделить кластера компаний (и вероятно, отдельно, компаний занимающихся облачными решениями), геймдев и приложения для разработки. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b8cb1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"sendpulse\",\n",
      "    \"etmc_exponenta\",\n",
      "    \"solarsecurity\",\n",
      "    \"3cx\",\n",
      "    \"izine\",\n",
      "    \"uml\",\n",
      "    \"fvdmedia\",\n",
      "    \"varonis\",\n",
      "    \"crowdsourcing\",\n",
      "    \"macloud\",\n",
      "    \"google_chrome\",\n",
      "    \"energy\",\n",
      "    \"phpshop\",\n",
      "    \"facebook\",\n",
      "    \"xakep\",\n",
      "    \"webassembly\",\n",
      "    \"1c\",\n",
      "    \"mootools\",\n",
      "    \"toster\",\n",
      "    \"acelab\",\n",
      "    \"cyberpunk\",\n",
      "    \"owasp\",\n",
      "    \"megafon\",\n",
      "    \"infowatch\",\n",
      "    \"bb-mobile\",\n",
      "    \"innoros\",\n",
      "    \"cgi\",\n",
      "    \"cloud_mts\",\n",
      "    \"tomhunter\",\n",
      "    \"build_automation\",\n",
      "    \"webo\",\n",
      "    \"desktops\",\n",
      "    \"devmail\",\n",
      "    \"t1_cloud\",\n",
      "    \"zwave\",\n",
      "    \"finolab\",\n",
      "    \"javascript\",\n",
      "    \"wearable_electronics\",\n",
      "    \"sales\",\n",
      "    \"haxe\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sample1 = random.sample(hubs.labels, 40)\n",
    "\n",
    "print(json.dumps(list(sample1), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2af37",
   "metadata": {},
   "source": [
    "Пробую намайнить позитивы из семпла выше (еще некоторые примеры группирую из списков выше):\n",
    "\n",
    "```[\n",
    "    [\n",
    "        \"branding\",\n",
    "        \"business_models\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"business-laws\",\n",
    "        \"business_models\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"research\",\n",
    "        \"patents\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"skyeng\",\n",
    "        \"learning_languages\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"sberbank\",\n",
    "        \"vtb\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"study\",\n",
    "        \"netologyru\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"DIY\",\n",
    "        \"raspberrypi\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "       \"html5\",\n",
    "       \"javascript\",\n",
    "       1 \n",
    "    ],\n",
    "    [\n",
    "        \"cpp\",\n",
    "        \"c\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"machine_learning\",\n",
    "        \"natural_language_processing\",\n",
    "        1\n",
    "    ],\n",
    "    [\n",
    "        \"mongodb\",\n",
    "        \"postgresql\",\n",
    "        1\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07ba6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = [\n",
    "    [\"branding\", \"business_models\", 1],\n",
    "    [\"business-laws\", \"business_models\", 1],\n",
    "    [\"research\", \"patents\", 1],\n",
    "    [\"skyeng\", \"learning_languages\", 1],\n",
    "    [\"sberbank\", \"vtb\", 1],\n",
    "    [\"study\", \"netologyru\", 1],\n",
    "    [\"DIY\", \"raspberrypi\", 1],\n",
    "    [\"html5\", \"javascript\", 1],\n",
    "    [\"cpp\", \"c\", 1],\n",
    "    [\"machine_learning\", \"natural_language_processing\", 1],\n",
    "    [\"mongodb\", \"postgresql\", 1],\n",
    "] + [\n",
    "    [\"microformats\", \"edge\", 0],\n",
    "    [\"masterkit\", \"medgadgets\", 1],\n",
    "    [\"asus\", \"social_networks\", 0],\n",
    "    [\"skyeng\", \"cpu\", 0],\n",
    "    [\"alconost\", \"metrotek\", 0.5],\n",
    "    [\"vertdider\", \"wunderfund\", 0.5],\n",
    "    [\"biology\", \"beeline\", 0],\n",
    "    [\"cloudsnn\", \"virtualization\", 0.5],\n",
    "    [\"plarium\", \"pixonic\", 1],\n",
    "    [\"compilers\", \"gadgets\", 0],\n",
    "    [\"study\", \"unisender\", 0],\n",
    "    [\"refactoring\", \"postgresql\", 0],\n",
    "    [\"itcompanies\", \"pay_system\", 0.5],\n",
    "    [\"owasp\", \"cybersport\", 0],\n",
    "    [\"antikvariat\", \"cloudsnn\", 0],\n",
    "    [\"image_processing\", \"html5\", 0],\n",
    "    [\"ie\", \"complete_code\", 0],\n",
    "    [\"learning_languages\", \"naumen\", 0],\n",
    "    [\"mongodb\", \"sport_programming\", 0],\n",
    "    [\"e-legion\", \"yandex_api\", 0],\n",
    "    [\"network_standarts\", \"mssql\", 0],\n",
    "    [\"3d_graphics\", \"xml\", 0],\n",
    "    [\"sberdevices\", \"1C\", 0.5],\n",
    "    [\"bitrix\", \"ozontech\", 0.5],\n",
    "    [\"regex\", \"getwear\", 0],\n",
    "    [\"vtb\", \"macloud\", 0],\n",
    "    [\"nano\", \"AJAX\", 0],\n",
    "    [\"skillbox\", \"biotech\", 0],\n",
    "    [\"energy\", \"youvend\", 0],\n",
    "    [\"sound\", \"hi\", 0],\n",
    "    [\"powershell\", \"vivaldi\", 0],\n",
    "    [\"growthhacking\", \"typography\", 0],\n",
    "    [\"biotech\", \"tablum\"],\n",
    "    [\"funcprog\", \"game_testing\", 0],\n",
    "    [\"ivideon\", \"doctrine\", 0],\n",
    "    [\"community_management\", \"analysis_design\", 0.5],\n",
    "    [\"kebrum\", \"domclick\", 0],\n",
    "    [\"1cloud\", \"notebooks\", 0],\n",
    "    [\"visual_programming\", \"arttel\", 0],\n",
    "    [\"sberbank\", \"ashmanov_net\", 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2afa4e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 13\n",
      "Negatives: 30\n",
      "Halfs: 7\n"
     ]
    }
   ],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "halfs = 0\n",
    "\n",
    "for pair in markup:\n",
    "    if len(pair) != 3:\n",
    "        continue\n",
    "    l, r, score = pair\n",
    "    positive += int(score == 1)\n",
    "    negative += int(score == 0)\n",
    "    halfs += int(score == 0.5)\n",
    "\n",
    "print(f\"Positives: {positive}\")\n",
    "print(f\"Negatives: {negative}\")\n",
    "print(f\"Halfs: {halfs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3904d",
   "metadata": {},
   "source": [
    "Посмотрим на самые частотные слова текстов статей для каждого хаба:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c88f8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'language', 'url', 'text_markdown', 'text_html', 'lead_markdown',\n",
       "       'lead_html', 'type', 'labels', 'original_author', 'original_url',\n",
       "       'time_published', 'author', 'title', 'statistics', 'hubs', 'flows',\n",
       "       'tags', 'reading_time', 'format', 'complexity', 'comments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f58b7e",
   "metadata": {},
   "source": [
    "Это не много, но это честная работа! (асессором)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03afcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_clusterization(\n",
    "    markup: list[list], clusters: dict[int : list[str]]\n",
    ") -> dict[str:float]:\n",
    "    markup = list(filter(lambda x: len(x) == 3 and x[2] != 0.5, markup))\n",
    "    inverse_clusters = dict(\n",
    "        [(value, key) for key, values in clusters.items() for value in values]\n",
    "    )\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    FPs = []\n",
    "    FNs = []\n",
    "    for pair in markup:\n",
    "        l, r, mark = pair\n",
    "        l_id, r_id = inverse_clusters[l], inverse_clusters[r]\n",
    "\n",
    "        if mark == 1 and l_id == r_id:\n",
    "            TP += 1\n",
    "        elif mark == 1 and l_id != r_id:\n",
    "            FN += 1\n",
    "            FNs.append((l, r))\n",
    "        elif mark == 0 and l_id == r_id:\n",
    "            FP += 1\n",
    "            FPs.append((l, r))\n",
    "        elif mark == 0 and l_id != r_id:\n",
    "            TN += 1\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TN\": TN,\n",
    "        \"accuracy\": (TP + TN) / len(markup),\n",
    "        \"false_positives\": FPs,\n",
    "        \"false_negatives\": FNs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134b2fb",
   "metadata": {},
   "source": [
    "Далее я буду пробовать различные способы кластеризации лейблов.\n",
    "Кластеризация состоит из двух частей:\n",
    "Embedder + Clusterization algorithm\n",
    "\n",
    "В качестве алгоритма кластеризации возьмем KMeans. \n",
    "А для эмбеддера рассмотрим несколько разных подходов:\n",
    "1. Вектор семплов\n",
    "2. Джаккардова мера похожести лейблов\n",
    "3. Эмбеддинги MiniLM \n",
    "4. Instructor (модель, у которой эмбеддинги конфигугрируются с помощью промпта)\n",
    "\n",
    "Так же для каждой модели подберем оптимальное множество кластеров (от 20 до 100).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "503093f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from habr_article_analyzer.clusters import TextClusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68c14d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_clusters_with_kmeans(\n",
    "    embeds, texts, n_clusters=[10, 25, 50, 75, 100], random_state=42, **kwargs\n",
    "):\n",
    "    best_res = {\"accuracy\": 0}\n",
    "    best_n = 0\n",
    "    for n in n_clusters:\n",
    "        clusters = KMeans(n_clusters=n, random_state=random_state).fit_predict(\n",
    "            embeds, **kwargs\n",
    "        )\n",
    "        clusters_mapping = TextClusterization._get_clusters(clusters, texts)\n",
    "        result = measure_clusterization(markup, clusters_mapping)\n",
    "        print(f\"n_clusters: {n}\")\n",
    "        print(f\"measurment: {result}\")\n",
    "        if result[\"accuracy\"] > best_res[\"accuracy\"]:\n",
    "            best_res = result\n",
    "            best_n = n\n",
    "\n",
    "    return best_res, best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79fc38d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 7,\n",
       " 'FP': 27,\n",
       " 'FN': 6,\n",
       " 'TN': 3,\n",
       " 'accuracy': 0.23255813953488372,\n",
       " 'false_positives': [('microformats', 'edge'),\n",
       "  ('skyeng', 'cpu'),\n",
       "  ('biology', 'beeline'),\n",
       "  ('study', 'unisender'),\n",
       "  ('refactoring', 'postgresql'),\n",
       "  ('owasp', 'cybersport'),\n",
       "  ('antikvariat', 'cloudsnn'),\n",
       "  ('ie', 'complete_code'),\n",
       "  ('learning_languages', 'naumen'),\n",
       "  ('mongodb', 'sport_programming'),\n",
       "  ('e-legion', 'yandex_api'),\n",
       "  ('network_standarts', 'mssql'),\n",
       "  ('3d_graphics', 'xml'),\n",
       "  ('regex', 'getwear'),\n",
       "  ('vtb', 'macloud'),\n",
       "  ('nano', 'AJAX'),\n",
       "  ('skillbox', 'biotech'),\n",
       "  ('energy', 'youvend'),\n",
       "  ('sound', 'hi'),\n",
       "  ('powershell', 'vivaldi'),\n",
       "  ('growthhacking', 'typography'),\n",
       "  ('funcprog', 'game_testing'),\n",
       "  ('ivideon', 'doctrine'),\n",
       "  ('kebrum', 'domclick'),\n",
       "  ('1cloud', 'notebooks'),\n",
       "  ('visual_programming', 'arttel'),\n",
       "  ('sberbank', 'ashmanov_net')],\n",
       " 'false_negatives': [('business-laws', 'business_models'),\n",
       "  ('research', 'patents'),\n",
       "  ('DIY', 'raspberrypi'),\n",
       "  ('html5', 'javascript'),\n",
       "  ('cpp', 'c'),\n",
       "  ('machine_learning', 'natural_language_processing')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from habr_article_analyzer.clusters import DatasetEmbedder\n",
    "\n",
    "dataset_kmeans = TextClusterization(\n",
    "    DatasetEmbedder(hubs), KMeans(n_clusters=50, random_state=42)\n",
    ")\n",
    "dataset_kmeans_clusters = dataset_kmeans.get_clusters(hubs.labels)\n",
    "measure_clusterization(markup, dataset_kmeans_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00f46289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 10\n",
      "measurment: {'TP': 12, 'FP': 30, 'FN': 1, 'TN': 0, 'accuracy': 0.27906976744186046, 'false_positives': [('microformats', 'edge'), ('asus', 'social_networks'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('compilers', 'gadgets'), ('study', 'unisender'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('antikvariat', 'cloudsnn'), ('image_processing', 'html5'), ('ie', 'complete_code'), ('learning_languages', 'naumen'), ('mongodb', 'sport_programming'), ('e-legion', 'yandex_api'), ('network_standarts', 'mssql'), ('3d_graphics', 'xml'), ('regex', 'getwear'), ('vtb', 'macloud'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('sound', 'hi'), ('powershell', 'vivaldi'), ('growthhacking', 'typography'), ('funcprog', 'game_testing'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('1cloud', 'notebooks'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('research', 'patents')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 11, 'FP': 28, 'FN': 2, 'TN': 2, 'accuracy': 0.3023255813953488, 'false_positives': [('microformats', 'edge'), ('asus', 'social_networks'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('study', 'unisender'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('antikvariat', 'cloudsnn'), ('ie', 'complete_code'), ('learning_languages', 'naumen'), ('mongodb', 'sport_programming'), ('e-legion', 'yandex_api'), ('network_standarts', 'mssql'), ('3d_graphics', 'xml'), ('regex', 'getwear'), ('vtb', 'macloud'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('sound', 'hi'), ('powershell', 'vivaldi'), ('growthhacking', 'typography'), ('funcprog', 'game_testing'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('1cloud', 'notebooks'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('html5', 'javascript'), ('cpp', 'c')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 7, 'FP': 27, 'FN': 6, 'TN': 3, 'accuracy': 0.23255813953488372, 'false_positives': [('microformats', 'edge'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('study', 'unisender'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('antikvariat', 'cloudsnn'), ('ie', 'complete_code'), ('learning_languages', 'naumen'), ('mongodb', 'sport_programming'), ('e-legion', 'yandex_api'), ('network_standarts', 'mssql'), ('3d_graphics', 'xml'), ('regex', 'getwear'), ('vtb', 'macloud'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('sound', 'hi'), ('powershell', 'vivaldi'), ('growthhacking', 'typography'), ('funcprog', 'game_testing'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('1cloud', 'notebooks'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('business-laws', 'business_models'), ('research', 'patents'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('cpp', 'c'), ('machine_learning', 'natural_language_processing')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 7, 'FP': 25, 'FN': 6, 'TN': 5, 'accuracy': 0.27906976744186046, 'false_positives': [('microformats', 'edge'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('image_processing', 'html5'), ('ie', 'complete_code'), ('learning_languages', 'naumen'), ('mongodb', 'sport_programming'), ('e-legion', 'yandex_api'), ('network_standarts', 'mssql'), ('3d_graphics', 'xml'), ('regex', 'getwear'), ('vtb', 'macloud'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('powershell', 'vivaldi'), ('growthhacking', 'typography'), ('funcprog', 'game_testing'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('1cloud', 'notebooks'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('business-laws', 'business_models'), ('research', 'patents'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('machine_learning', 'natural_language_processing')]}\n",
      "n_clusters: 100\n",
      "measurment: {'TP': 4, 'FP': 23, 'FN': 9, 'TN': 7, 'accuracy': 0.2558139534883721, 'false_positives': [('microformats', 'edge'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('owasp', 'cybersport'), ('ie', 'complete_code'), ('learning_languages', 'naumen'), ('mongodb', 'sport_programming'), ('e-legion', 'yandex_api'), ('network_standarts', 'mssql'), ('3d_graphics', 'xml'), ('regex', 'getwear'), ('vtb', 'macloud'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('powershell', 'vivaldi'), ('growthhacking', 'typography'), ('funcprog', 'game_testing'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('1cloud', 'notebooks'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('cpp', 'c'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql')]}\n",
      "best n: 25\n",
      "best_res: {'TP': 11, 'FP': 28, 'FN': 2, 'TN': 2, 'accuracy': 0.3023255813953488, 'false_positives': [('microformats', 'edge'), ('asus', 'social_networks'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('study', 'unisender'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('antikvariat', 'cloudsnn'), ('ie', 'complete_code'), ('learning_languages', 'naumen'), ('mongodb', 'sport_programming'), ('e-legion', 'yandex_api'), ('network_standarts', 'mssql'), ('3d_graphics', 'xml'), ('regex', 'getwear'), ('vtb', 'macloud'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('sound', 'hi'), ('powershell', 'vivaldi'), ('growthhacking', 'typography'), ('funcprog', 'game_testing'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('1cloud', 'notebooks'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('html5', 'javascript'), ('cpp', 'c')]}\n"
     ]
    }
   ],
   "source": [
    "best_res, best_n = get_best_clusters_with_kmeans(\n",
    "    DatasetEmbedder(hubs).encode(hubs.labels), hubs.labels\n",
    ")\n",
    "print(f\"best n: {best_n}\")\n",
    "print(f\"best_res: {best_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f8691f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergiyar/habr-article-analyzer/env/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:2462: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TP': 4,\n",
       " 'FP': 3,\n",
       " 'FN': 9,\n",
       " 'TN': 27,\n",
       " 'accuracy': 0.7209302325581395,\n",
       " 'false_positives': [('refactoring', 'postgresql'),\n",
       "  ('owasp', 'cybersport'),\n",
       "  ('mongodb', 'sport_programming')],\n",
       " 'false_negatives': [('branding', 'business_models'),\n",
       "  ('business-laws', 'business_models'),\n",
       "  ('research', 'patents'),\n",
       "  ('skyeng', 'learning_languages'),\n",
       "  ('study', 'netologyru'),\n",
       "  ('DIY', 'raspberrypi'),\n",
       "  ('html5', 'javascript'),\n",
       "  ('mongodb', 'postgresql'),\n",
       "  ('masterkit', 'medgadgets')]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from habr_article_analyzer.clusters import JaccardLabelsEmbedder\n",
    "\n",
    "jaccard_kmeans = TextClusterization(\n",
    "    JaccardLabelsEmbedder(hubs), KMeans(n_clusters=50, random_state=42)\n",
    ")\n",
    "jaccard_kmeans_clusters = jaccard_kmeans.get_clusters(hubs.labels)\n",
    "measure_clusterization(markup, jaccard_kmeans_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9612605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergiyar/habr-article-analyzer/env/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:2462: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 10\n",
      "measurment: {'TP': 6, 'FP': 11, 'FN': 7, 'TN': 19, 'accuracy': 0.5813953488372093, 'false_positives': [('microformats', 'edge'), ('biology', 'beeline'), ('owasp', 'cybersport'), ('ie', 'complete_code'), ('mongodb', 'sport_programming'), ('regex', 'getwear'), ('nano', 'AJAX'), ('powershell', 'vivaldi'), ('funcprog', 'game_testing'), ('kebrum', 'domclick'), ('1cloud', 'notebooks')], 'false_negatives': [('branding', 'business_models'), ('research', 'patents'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 5, 'FP': 14, 'FN': 8, 'TN': 16, 'accuracy': 0.4883720930232558, 'false_positives': [('microformats', 'edge'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('mongodb', 'sport_programming'), ('nano', 'AJAX'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('powershell', 'vivaldi'), ('ivideon', 'doctrine'), ('kebrum', 'domclick'), ('visual_programming', 'arttel'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('cpp', 'c'), ('mongodb', 'postgresql')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 4, 'FP': 3, 'FN': 9, 'TN': 27, 'accuracy': 0.7209302325581395, 'false_positives': [('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('mongodb', 'sport_programming')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 5, 'FP': 2, 'FN': 8, 'TN': 28, 'accuracy': 0.7674418604651163, 'false_positives': [('regex', 'getwear'), ('powershell', 'vivaldi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "n_clusters: 100\n",
      "measurment: {'TP': 5, 'FP': 3, 'FN': 8, 'TN': 27, 'accuracy': 0.7441860465116279, 'false_positives': [('regex', 'getwear'), ('powershell', 'vivaldi'), ('sberbank', 'ashmanov_net')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "best n: 75\n",
      "best_res: {'TP': 5, 'FP': 2, 'FN': 8, 'TN': 28, 'accuracy': 0.7674418604651163, 'false_positives': [('regex', 'getwear'), ('powershell', 'vivaldi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n"
     ]
    }
   ],
   "source": [
    "best_res, best_n = get_best_clusters_with_kmeans(\n",
    "    JaccardLabelsEmbedder(hubs).encode(hubs.labels), hubs.labels\n",
    ")\n",
    "print(f\"best n: {best_n}\")\n",
    "print(f\"best_res: {best_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4ec2262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 7,\n",
       " 'FP': 0,\n",
       " 'FN': 6,\n",
       " 'TN': 30,\n",
       " 'accuracy': 0.8604651162790697,\n",
       " 'false_positives': [],\n",
       " 'false_negatives': [('research', 'patents'),\n",
       "  ('skyeng', 'learning_languages'),\n",
       "  ('sberbank', 'vtb'),\n",
       "  ('study', 'netologyru'),\n",
       "  ('DIY', 'raspberrypi'),\n",
       "  ('mongodb', 'postgresql')]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub.utils import disable_progress_bars\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "disable_progress_bars()\n",
    "\n",
    "minillm_kmeans = TextClusterization(\n",
    "    SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"),\n",
    "    KMeans(n_clusters=50, random_state=42),\n",
    ")\n",
    "minillm_kmeans_clusters = minillm_kmeans.get_clusters(hubs.labels)\n",
    "measure_clusterization(markup, minillm_kmeans_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9a34226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 10\n",
      "measurment: {'TP': 6, 'FP': 3, 'FN': 7, 'TN': 27, 'accuracy': 0.7674418604651163, 'false_positives': [('skillbox', 'biotech'), ('sound', 'hi'), ('ivideon', 'doctrine')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('skyeng', 'learning_languages'), ('study', 'netologyru'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 6, 'FP': 2, 'FN': 7, 'TN': 28, 'accuracy': 0.7906976744186046, 'false_positives': [('e-legion', 'yandex_api'), ('energy', 'youvend')], 'false_negatives': [('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 7, 'FP': 0, 'FN': 6, 'TN': 30, 'accuracy': 0.8604651162790697, 'false_positives': [], 'false_negatives': [('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('mongodb', 'postgresql')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 5, 'FP': 0, 'FN': 8, 'TN': 30, 'accuracy': 0.813953488372093, 'false_positives': [], 'false_negatives': [('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "n_clusters: 100\n",
      "measurment: {'TP': 2, 'FP': 0, 'FN': 11, 'TN': 30, 'accuracy': 0.7441860465116279, 'false_positives': [], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets')]}\n",
      "best n: 50\n",
      "best_res: {'TP': 7, 'FP': 0, 'FN': 6, 'TN': 30, 'accuracy': 0.8604651162790697, 'false_positives': [], 'false_negatives': [('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('mongodb', 'postgresql')]}\n"
     ]
    }
   ],
   "source": [
    "best_res, best_n = get_best_clusters_with_kmeans(\n",
    "    SentenceTransformer(\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    ).encode(hubs.labels),\n",
    "    hubs.labels,\n",
    ")\n",
    "print(f\"best n: {best_n}\")\n",
    "print(f\"best_res: {best_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efe11c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name hkunlp/instructor-large. Creating a new one with mean pooling.\n",
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TP': 2,\n",
       " 'FP': 1,\n",
       " 'FN': 11,\n",
       " 'TN': 29,\n",
       " 'accuracy': 0.7209302325581395,\n",
       " 'false_positives': [('growthhacking', 'typography')],\n",
       " 'false_negatives': [('branding', 'business_models'),\n",
       "  ('business-laws', 'business_models'),\n",
       "  ('research', 'patents'),\n",
       "  ('skyeng', 'learning_languages'),\n",
       "  ('sberbank', 'vtb'),\n",
       "  ('study', 'netologyru'),\n",
       "  ('DIY', 'raspberrypi'),\n",
       "  ('html5', 'javascript'),\n",
       "  ('mongodb', 'postgresql'),\n",
       "  ('masterkit', 'medgadgets'),\n",
       "  ('plarium', 'pixonic')]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "instructor_kmeans = TextClusterization(\n",
    "    INSTRUCTOR(\"hkunlp/instructor-large\"), KMeans(n_clusters=50, random_state=42)\n",
    ")\n",
    "text_instruction_pairs = [\n",
    "    [\n",
    "        \"Represent the Programming Hub category for company or technology clustering:\",\n",
    "        label,\n",
    "    ]\n",
    "    for label in hubs.labels\n",
    "]\n",
    "instructor_kmeans_clusters = instructor_kmeans.get_clusters(text_instruction_pairs)\n",
    "instructor_kmeans_clusters = dict(\n",
    "    (key, [value[1] for value in values])\n",
    "    for key, values in instructor_kmeans_clusters.items()\n",
    ")\n",
    "measure_clusterization(markup, instructor_kmeans_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50babcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name hkunlp/instructor-large. Creating a new one with mean pooling.\n",
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_res, best_n = get_best_clusters_with_kmeans(\u001b[43mINSTRUCTOR\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhkunlp/instructor-large\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_instruction_pairs\u001b[49m\u001b[43m)\u001b[49m, hubs.labels)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest n: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest_res: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_res\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/InstructorEmbedding/instructor.py:539\u001b[39m, in \u001b[36mINSTRUCTOR.encode\u001b[39m\u001b[34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[39m\n\u001b[32m    536\u001b[39m features = batch_to_device(features, device)\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_value == \u001b[33m'\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    542\u001b[39m         embeddings = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1932\u001b[39m, in \u001b[36mT5EncoderModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1907\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1908\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[32m   1909\u001b[39m \u001b[33;03m    Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1928\u001b[39m \u001b[33;03m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[32m   1929\u001b[39m \u001b[33;03m```\"\"\"\u001b[39;00m\n\u001b[32m   1930\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1932\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1935\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1938\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1100\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m   1098\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1118\u001b[39m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:687\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    672\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    685\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    686\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m     hidden_states = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    698\u001b[39m     attention_outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:603\u001b[39m, in \u001b[36mT5LayerSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    600\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    601\u001b[39m ):\n\u001b[32m    602\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n\u001b[32m    614\u001b[39m     outputs = (hidden_states,) + attention_output[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:513\u001b[39m, in \u001b[36mT5Attention.forward\u001b[39m\u001b[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    511\u001b[39m     value_states = curr_past_key_value.layers[\u001b[38;5;28mself\u001b[39m.layer_idx].values\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     value_states = \u001b[38;5;28mself\u001b[39m.v(current_states)\n\u001b[32m    515\u001b[39m     key_states = key_states.view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.key_value_proj_dim).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_res, best_n = get_best_clusters_with_kmeans(\n",
    "    INSTRUCTOR(\"hkunlp/instructor-large\").encode(text_instruction_pairs), hubs.labels\n",
    ")\n",
    "print(f\"best n: {best_n}\")\n",
    "print(f\"best_res: {best_res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31284b25",
   "metadata": {},
   "source": [
    "Попробуем еще поподбирать промпт для модели Instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035d26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name hkunlp/instructor-large. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "def choose_best_prompt(\n",
    "    prompts: list[str], texts: list[str], model=INSTRUCTOR(\"hkunlp/instructor-large\")\n",
    "):\n",
    "    best_res = {\"accuracy\": 0}\n",
    "    best_n = 0\n",
    "    best_prompt = \"\"\n",
    "    for prompt in prompts:\n",
    "        text_instruction_pairs = [[prompt, label] for label in texts]\n",
    "        b_res, b_n = get_best_clusters_with_kmeans(\n",
    "            model.encode(text_instruction_pairs), texts\n",
    "        )\n",
    "\n",
    "        if b_res[\"accuracy\"] > best_res[\"accuracy\"]:\n",
    "            best_res = b_res\n",
    "            best_n = b_n\n",
    "            best_prompt = prompt\n",
    "\n",
    "    return best_res, best_n, best_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896fe232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 10\n",
      "measurment: {'TP': 5, 'FP': 7, 'FN': 8, 'TN': 23, 'accuracy': 0.6511627906976745, 'false_positives': [('microformats', 'edge'), ('biology', 'beeline'), ('study', 'unisender'), ('regex', 'getwear'), ('energy', 'youvend'), ('sound', 'hi'), ('1cloud', 'notebooks')], 'false_negatives': [('branding', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 3, 'FP': 2, 'FN': 10, 'TN': 28, 'accuracy': 0.7209302325581395, 'false_positives': [('microformats', 'edge'), ('sound', 'hi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 3, 'FP': 2, 'FN': 10, 'TN': 28, 'accuracy': 0.7209302325581395, 'false_positives': [('sound', 'hi'), ('growthhacking', 'typography')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 2, 'FP': 1, 'FN': 11, 'TN': 29, 'accuracy': 0.7209302325581395, 'false_positives': [('biology', 'beeline')], 'false_negatives': [('branding', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 100\n",
      "measurment: {'TP': 1, 'FP': 0, 'FN': 12, 'TN': 30, 'accuracy': 0.7209302325581395, 'false_positives': [], 'false_negatives': [('branding', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('cpp', 'c'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 10\n",
      "measurment: {'TP': 5, 'FP': 6, 'FN': 8, 'TN': 24, 'accuracy': 0.6744186046511628, 'false_positives': [('skyeng', 'cpu'), ('compilers', 'gadgets'), ('owasp', 'cybersport'), ('network_standarts', 'mssql'), ('energy', 'youvend'), ('sound', 'hi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('machine_learning', 'natural_language_processing'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 3, 'FP': 4, 'FN': 10, 'TN': 26, 'accuracy': 0.6744186046511628, 'false_positives': [('network_standarts', 'mssql'), ('skillbox', 'biotech'), ('sound', 'hi'), ('growthhacking', 'typography')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 1, 'FP': 1, 'FN': 12, 'TN': 29, 'accuracy': 0.6976744186046512, 'false_positives': [('energy', 'youvend')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('cpp', 'c'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 1, 'FP': 0, 'FN': 12, 'TN': 30, 'accuracy': 0.7209302325581395, 'false_positives': [], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 100\n",
      "measurment: {'TP': 1, 'FP': 0, 'FN': 12, 'TN': 30, 'accuracy': 0.7209302325581395, 'false_positives': [], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 10\n",
      "measurment: {'TP': 2, 'FP': 3, 'FN': 11, 'TN': 27, 'accuracy': 0.6744186046511628, 'false_positives': [('skyeng', 'cpu'), ('energy', 'youvend'), ('growthhacking', 'typography')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 3, 'FP': 5, 'FN': 10, 'TN': 25, 'accuracy': 0.6511627906976745, 'false_positives': [('skyeng', 'cpu'), ('refactoring', 'postgresql'), ('owasp', 'cybersport'), ('energy', 'youvend'), ('sound', 'hi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 2, 'FP': 2, 'FN': 11, 'TN': 28, 'accuracy': 0.6976744186046512, 'false_positives': [('microformats', 'edge'), ('sound', 'hi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 4, 'FP': 1, 'FN': 9, 'TN': 29, 'accuracy': 0.7674418604651163, 'false_positives': [('regex', 'getwear')], 'false_negatives': [('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 100\n",
      "measurment: {'TP': 4, 'FP': 1, 'FN': 9, 'TN': 29, 'accuracy': 0.7674418604651163, 'false_positives': [('growthhacking', 'typography')], 'false_negatives': [('branding', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 10\n",
      "measurment: {'TP': 4, 'FP': 7, 'FN': 9, 'TN': 23, 'accuracy': 0.627906976744186, 'false_positives': [('microformats', 'edge'), ('skyeng', 'cpu'), ('biology', 'beeline'), ('skillbox', 'biotech'), ('energy', 'youvend'), ('sound', 'hi'), ('growthhacking', 'typography')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 25\n",
      "measurment: {'TP': 3, 'FP': 3, 'FN': 10, 'TN': 27, 'accuracy': 0.6976744186046512, 'false_positives': [('microformats', 'edge'), ('biology', 'beeline'), ('sound', 'hi')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 50\n",
      "measurment: {'TP': 3, 'FP': 2, 'FN': 10, 'TN': 28, 'accuracy': 0.7209302325581395, 'false_positives': [('sound', 'hi'), ('growthhacking', 'typography')], 'false_negatives': [('branding', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 75\n",
      "measurment: {'TP': 2, 'FP': 2, 'FN': 11, 'TN': 28, 'accuracy': 0.6976744186046512, 'false_positives': [('microformats', 'edge'), ('biology', 'beeline')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "n_clusters: 100\n",
      "measurment: {'TP': 1, 'FP': 1, 'FN': 12, 'TN': 29, 'accuracy': 0.6976744186046512, 'false_positives': [('biology', 'beeline')], 'false_negatives': [('branding', 'business_models'), ('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('DIY', 'raspberrypi'), ('html5', 'javascript'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n"
     ]
    }
   ],
   "source": [
    "best_res, best_n, best_prompt = choose_best_prompt(\n",
    "    prompts=[\n",
    "        \"Represent the Programming Hub category for clustering:\",\n",
    "        \"Represent the Programming Hub category for company or technology clustering:\",\n",
    "        \"Represent the Programming Hub category for company or technology or activity clustering:\",\n",
    "        \"Represent the Programming Hub category for company or technology or profession clustering:\",\n",
    "    ],\n",
    "    texts=hubs.labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ddf947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_res: {'TP': 4, 'FP': 1, 'FN': 9, 'TN': 29, 'accuracy': 0.7674418604651163, 'false_positives': [('regex', 'getwear')], 'false_negatives': [('business-laws', 'business_models'), ('research', 'patents'), ('skyeng', 'learning_languages'), ('sberbank', 'vtb'), ('study', 'netologyru'), ('machine_learning', 'natural_language_processing'), ('mongodb', 'postgresql'), ('masterkit', 'medgadgets'), ('plarium', 'pixonic')]}\n",
      "best_n: 75\n",
      "best_prompt: Represent the Programming Hub category for company or technology or activity clustering:\n"
     ]
    }
   ],
   "source": [
    "print(f\"best_res: {best_res}\")\n",
    "print(f\"best_n: {best_n}\")\n",
    "print(f\"best_prompt: {best_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name hkunlp/instructor-large. Creating a new one with mean pooling.\n",
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{36: ['1C', '1c', '1cair', '2can', '2gis', '7days', '8812', 'Atlassian', 'Centrobit', 'Competentum', 'DIY', 'Hadoop', 'Licel', 'accessibility', 'acelab', 'acer', 'action360', 'acumatica', 'adapty', 'agile', 'aktiv-company', 'alawar', 'alee', 'alfa', 'algorithms', 'aligntechnology', 'allcorrect', 'altergeo', 'altnet', 'ambar', 'amd', 'animation', 'anychart', 'apacer', 'apache', 'api', 'aquaphor', 'arenadata', 'asp', 'asterisk', 'asus', 'atlas', 'auriga', 'avi', 'azure', 'badoo', 'bar', 'barsgroup', 'bastion', 'beeline', 'beget', 'big', 'bigdata', 'bigdataplatform', 'biggo', 'bonjoin', 'brave', 'c', 'cackle', 'canon', 'caravan', 'cardberry', 'career', 'cartaxi', 'celecom', 'changeagain', 'changellenge', 'chateam', 'cian', 'cit', 'cleantalk', 'cleverence', 'click', 'cloverr', 'clovertel', 'clrium', 'codefest', 'colobridge', 'comagic', 'compilers', 'constanta', 'contell', 'contentai', 'context', 'convead', 'converse', 'conversion', 'copiny', 'copyright', 'corel', 'cpp', 'cpu', 'crazydev', 'crazypanda', 'crm', 'crossover', 'crowdsourcing', 'crypto', 'cryptocurrency', 'csharp', 'cubrid', 'custis', 'cybersport', 'databoom', 'datacore', 'datanomica', 'datapro', 'debug', 'deliveryclub', 'design', 'displays', 'dou', 'eclipse', 'edge', 'eleven', 'energy', 'est', 'euler2012', 'first', 'forth', 'gToday', 'gae', 'gai_company', 'garmin', 'gearbest', 'geekfactor', 'geekfamily', 'generateclub', 'generations', 'generatum', 'genue', 'geo', 'getmatch', 'getwear', 'gigant', 'git', 'github', 'globalsign', 'glowbyte', 'go', 'goto', 'greasemonkey', 'gridgain', 'growthhacking', 'grrow', 'gtk', 'habr', 'habrareviews', 'hackathons', 'haskell', 'hc', 'health', 'hfhe', 'hh', 'hi', 'hiconversion', 'highscreen', 'hiper', 'history', 'hola', 'hordes', 'hotreader', 'hp', 'hpe', 'human-way', 'hygger', 'inDrive', 'infolust', 'inforion', 'infowatch', 'inside', 'instudies', 'intel', 'intellin', 'intersect', 'interviews', 'it', 'itcompanies', 'itforyou', 'itpark', 'java', 'javarush', 'jelastic', 'journalsua', 'julia', 'krible', 'langled', 'larian', 'latera', 'leader-id', 'lendwings', 'levelpride', 'lg', 'lib', 'lifehacks', 'lifestreet', 'lightmap', 'lineate', 'linka', 'linux', 'lisp', 'listpr', 'livestreet', 'localization', 'lokalise', 'lua', 'makeright', 'manychat', 'materialise', 'maths', 'meople', 'mercurial', 'metabar', 'mono', 'myasterisk', 'nativepc', 'navigine', 'near', 'net', 'numdes', 'oop', 'open_source', 'opendata', 'openshift', 'openstat', 'openstreetmap', 'panda', 'paragon', 'parallels', 'parking-old', 'pdf', 'perl', 'photo', 'physics', 'picktime', 'pilot', 'pingwin', 'planeta', 'plarium', 'pm', 'posthawk', 'powershell', 'prequel', 'presentation', 'primetalk', 'programming', 'puzzleenglish', 'python', 'railsclub', 'rainbow', 'raspberrypi', 'read', 'realtrac', 'refactoring', 'regex', 'research', 'ridewithme', 'roundlake', 'ruby', 'safedata', 'san', 'scala', 'scancode', 'scrumguides', 'scrumtrek', 'searchinform', 'selectel', 'shalb', 'sharepoint', 'shells', 'silverlight', 'simpleone', 'simpleweek', 'singularis', 'smallbasic', 'smalltalk', 'softwarepeople', 'soloten', 'sourcetalk', 'space', 'spam', 'specialist', 'speechpro', 'spice', 'splitmetrics', 'statistics', 'studier', 'study', 'superjob', 'synaps', 'synergy', 'synesis', 'tablum', 'take5', 'technocup', 'tion', 'token2', 'topic', 'topstory', 'towave', 'tracksflow', 'transport', 'twins', 'typeable', 'ui', 'urban', 'usetech', 'vim', 'vs', 'wallarm', 'waves', 'wayray', 'weban', 'webgl', 'weblancer', 'webo', 'westcomp', 'whalesburg', 'wifire', 'wolfram', 'workle', 'wrike'], 1: ['1c_club', 'BrandMaker', 'UXDepot', 'action_script', 'analysis_design', 'at_consulting', 'bcs_company', 'brandanalytics', 'branding', 'business_models', 'community_management', 'complete_code', 'contentmarketing', 'culture_home', 'data_compression', 'data_engineering', 'data_mining', 'data_recovery', 'data_visualization', 'data_warehouse', 'datarecovery', 'datawiz', 'designboom', 'dev_management', 'display_adv', 'graph_design', 'habr_career', 'hr_management', 'icon_partners', 'icontext', 'image_processing', 'imagecms', 'infoculture', 'infographics', 'inline_tech', 'internetmarketing', 'it_bigraphy', 'it_home', 'it_people', 'itbrand', 'logomachine', 'machine_learning', 'map_api', 'mbaconsult', 'metaquotes', 'new_hr', 'open_technologies', 'popular_science', 'processing_lang', 'related_media', 'se_blog', 'search_technologies', 'smart_it', 'system_programming', 'technical_writing', 'typography', 'uidesign', 'usability', 'usabilitylab', 'vector_graphics', 'visiology', 'visual-science', 'visual_programming', 'web_design', 'wp_dev'], 28: ['1c_game_studios', 'anvilgames', 'apeigaming', 'game_design', 'game_monetization', 'game_promotion', 'game_testing', 'gamedev', 'gameinsight', 'gamersweb', 'games', 'gamesmailru', 'gametrix', 'gamingmedia', 'kamagames', 'logic_games', 'mygames', 'niceplay_games', 'spilgames', 'starnigames'], 22: ['1cloud', 'ActiveCloud', 'activecloudru', 'aws', 'cloud4y', 'cloud_clout', 'cloud_computing', 'cloud_mts', 'cloud_services', 'cloudcastle', 'cloudera', 'cloudlayar', 'cloudone', 'cloudsnn', 'cloudstats', 'dentalcloud', 'lpcloud', 'macloud', 'mclouds', 'ncloudtech', 'ringcloud', 'sbercloud', 'skybase', 't1_cloud', 'unicloud'], 64: ['2035_university', 'academy', 'courson', 'dataschool', 'edu', 'edu-craft', 'educraft', 'foxcourse', 'golovachcourses', 'kidsreview', 'learning_languages', 'learzing', 'legoeducation', 'navigatorcampus', 'opentechedu', 'taucraft', 'teachbase', 'tutoronline', 'westudyin'], 47: ['2pay', 'billing', 'cashmyvisit', 'chronopay', 'epayments', 'epayservice', 'it_monetization', 'moneypipe', 'pay_system', 'payanyway', 'paymentwall', 'payoneer', 'payonline', 'paysto', 'payweb', 'pocketbook', 'simplepay', 'vivid_money', 'wayforpay', 'web_monetization', 'web_payment_ru', 'webmoney', 'zpayment'], 55: ['2thhappypm', '88005554448', 'Get_experts', 'Sailfish_dev', 'Social_Discovery_Group', 'admitad_gmbh', 'akbarsdigital', 'allmychanges', 'angels-it', 'anker_innovations', 'antiplagiat', 'aviasales', 'baikalelectron', 'bashnipineft', 'belayaraduga', 'boodet_online', 'callhelper_kitmedia', 'canopus_it', 'code_wtf', 'db_admins', 'digitaloctober', 'dva_myacha', 'elixirphoenix', 'fix_price', 'gagar_in', 'gazpromneft', 'generations-2014', 'gilalgorithms', 'globatel_ltd', 'gopherconru', 'gorky_ai', 'grambo_pi', 'greenfield_project', 'groovy_grails', 'happypregnancy', 'hidemy_name', 'i_am_advertising', 'iladaruli24', 'incube-ane', 'infotecs_official', 'instant_messaging', 'instinctools', 'it_immigration', 'it_poselok', 'javame_dev', 'just_ai', 'kingston_technology', 'kryptonite', 'kyocera_ds', 'leadersofdigital', 'lenta_utkonos_tech', 'leroy_merlin', 'lesallay_dialog', 'luxoft_personnel', 'mirantis_openstack', 'mneniya_pro', 'moysklad', 'mr_gefest', 'my_wins', 'natural_language_processing', 'npo_rapira_ltd', 'odin_ingram_micro', 'oppodigital', 'oversun-mercury', 'oversun_media', 'papabubadiop', 'phoenix_contact', 'pik_digital', 'praktikum_yandex', 'profit_partner', 'rambler_and_co', 'realno_deshevle', 's_admin', 'samokat_tech', 'samsung', 'sanuel', 'scienceman_events', 'sdelki24', 'shablonarium', 'sibur_official', 'speakasap', 'sport_programming', 'squeek', 'surprise_me', 'sys_admin', 'take_the_cake', 'tech_events', 'thecommentor', 'tiktokcoach', 'timera_inc', 'tizen_dev', 'tottoli_gsm', 'treolan_treolan', 'unistar_digital', 'voice_interfaces', 'yandex_praktikum', 'yasno_tv'], 39: ['360totalsecurity', 'angarasecurity', 'crowdsec', 'cyberprotect', 'cybersafe', 'ddosguard', 'digitalsecuritylab', 'infosecurity', 'jethackers', 'securitycode', 'solarsecurity', 'swordfish_security', 'uc_itsecurity', 'virgilsecurity'], 12: ['3cx', '3d_graphics', '3dquality', '3dtool', '3rdman', 'business3', 'cvetmir3d', 'thirdpin', 'top3dshop', 'trinion', 'trinity', 'twin3d', 'typo3', 'unity3d', 'web3_tech', 'what3words'], 20: ['3d-printers', 'advantech', 'alfarobotics', 'analogbytes', 'aquanetworks', 'arduino', 'artificial_intelligence', 'automacon', 'besmart', 'build_automation', 'cad_cam', 'circuit-design', 'cityscanner', 'contactless', 'controllers', 'coptertime', 'cubicrobotics', 'cyberpunk', 'depocomputers', 'distributed_systems', 'dreamindustries', 'droider', 'engineering_systems', 'engineyard', 'factory5', 'farminers', 'futurenow', 'geo_systems', 'happyfarm', 'home_automation', 'industrial_control_system', 'inforotor', 'jetinfosystems', 'lasers', 'littlebeetle', 'logicmachine', 'ls_computers', 'madrobots', 'minirobot', 'multicopters', 'parallel_programming', 'recognitor', 'redmadrobot', 'reverse-engineering', 'robo_dev', 'robogazon', 'robohunter', 'robot', 'robotgeeks', 'robouniver', 'rocketcallback', 'sci-fi', 'sensecognition', 'simtechdev', 'simulationsystems', 'sitronics_group', 'skillfactory', 'smartengines', 'smartliving', 'smartprogress', 'smartreading', 'solvermate', 'supercomputers', 'syncdriver', 'technotronics', 'technoworks', 'true_engineering', 'vsrobotics', 'wizartech', 'zerotech', 'zwave'], 21: ['3mrussia', 'LG_Russia', 'advertone_ru', 'aliexpress_russia', 'byrobot_russia', 'cansonic_russia', 'cpp_russia', 'dnevnik_ru', 'fpi_russia', 'gemaltorussia', 'kmeleon_rus', 'panasonic_russia', 'pgdayrussia', 'qualcomm_russia', 'redhatrussia', 'rusonyx', 'russian_rehab_industry', 'russol', 'ruswizards', 'sfe_ru', 'sports_ru', 'tp_link_russia', 'wikimedia_ru'], 53: ['404HUB', '404fest', 'adminvps', 'advanserv', 'airbnb', 'alexhost', 'atosservers', 'backendless', 'bigdatahosting', 'browser_extensions', 'browsers', 'callbackkiller', 'cgi', 'client_side_optimization', 'comet-server', 'css', 'devops', 'host-tracker', 'hostco', 'hosting', 'hosting-cafe', 'hostkey', 'htmlacademy', 'ie', 'iis', 'kickidler', 'kingservers', 'kubernetes', 'nginx', 'office365dev', 'offlinecrm', 'onlinepro', 'peterhost', 'puppet', 'saas', 'scalablesolutions', 'seo', 'server_side_optimization', 'serverclub', 'serverless', 'servermall', 'serverspace', 'sprinthost', 'ucbrowser', 'web_ready', 'webassembly', 'webasyst', 'webcreds', 'webdev', 'webhostinginc', 'webprofessionals', 'webprojects', 'webzilla'], 16: ['4glaza', 'amperka', 'banderolka', 'beelinekz', 'chestnyznak', 'cyberleninka', 'doktelecom', 'dreamkas', 'iconoskaz', 'ipmatika', 'kabanchik', 'karuna', 'kaspersky', 'kebrum', 'kksng', 'knopka', 'kodeks', 'kodicms', 'kokoc_group', 'kolesa', 'kolibrios', 'komponenta', 'komtet', 'koshelek', 'kotlin', 'krista', 'krob', 'ksrk', 'lektorium', 'matreshka', 'niisokb', 'oblakoteka', 'okkamgroup', 'okko', 'otkritie', 'polaz', 'postnauka', 'postok', 'reklamer', 'robotpashka', 'rozetked', 'sartelekom', 'skbkontur', 'tapki', 'tochka', 'usedesk', 'zadachki', 'zadarma', 'zarlaw', 'zorkamobi'], 3: ['5518studios', 'businessstudio', 'closet', 'coworking14', 'devimteam', 'eurostudio', 'floor796', 'garage8', 'lesta_studio', 'localkitchen', 'office', 'office_partner', 'productivity_inside', 'pvs-studio', 'room_8_studio', 'surfstudio', 'teamlab', 'workzilla'], 62: ['65apps', 'Xamarin', 'android_dev', 'appgranula', 'apphud', 'appnestic', 'appodeal', 'apps4all', 'apps_design', 'apptractor', 'avoapp', 'calltouch', 'doubletapp', 'getintent', 'gradle', 'my-apps', 'planetiphone', 'tablets', 'touchinstinct', 'uipath'], 30: ['8not', 'ancort', 'apoi', 'atol', 'cleverpumpkin', 'commandspot', 'communigatepro', 'compot2022', 'creatent', 'datafort', 'dateprog', 'galtsystems', 'getmeit', 'gett', 'godot', 'haulmont', 'hottaby4', 'kato', 'lakhtacenter', 'lanit', 'livetyping', 'meedget', 'mipt', 'muk', 'pechkin', 'pentestit', 'pgk', 'philtech', 'pirogov', 'piter', 'plesk', 'poborchy', 'pochtafi', 'pochtoy', 'polyglot', 'posttech', 'postuf', 'potok', 'ppr', 'preply', 'prestel', 'prhero', 'proactivity', 'productpm', 'profyclub', 'prolan', 'promwad', 'prostoy', 'proto', 'prototyping', 'prototypster', 'provectus', 'prt', 'pt', 'ptysh', 'pult', 'pushall', 'scanport', 'synchronet', 'taxolet', 'textocat', 'witget', 'zfort'], 37: ['AJAX', 'allcountjs', 'angular', 'coffeescript', 'derbyjs', 'emberjs', 'extjs', 'html5', 'jarvisjs', 'javascript', 'jquery', 'meteorjs', 'mootools', 'nodejs', 'reactjs', 'renderjs', 'sveltejs', 'typescript', 'vuejs'], 56: ['AflexDistribution', 'afidistribution', 'flex', 'flexbby', 'friflex', 'neoflex'], 15: ['Chemistry', 'ZeptoLab', 'adlabs', 'atlasbiomed', 'bincubator', 'biocad', 'biology', 'biotech', 'biotrack', 'datalaboratory', 'deiteriylab', 'distributedlab', 'dmlabs', 'finolab', 'fintechlab', 'gcorelabs', 'gitlab', 'innovationlab', 'innovationlabs', 'labview', 'makeitlab', 'newprolab', 'ntechlab', 'onlab', 'onlinepatent', 'palitrumlab', 'patents', 'performance_lab', 'rtlabs', 'speclab', 'sportmaster_lab', 'stm_labs'], 63: ['Ciklum', 'Nekki', 'ackuna', 'anetika', 'comptek', 'geltek', 'genotek', 'itentika', 'kelnik', 'meanotek', 'nekaka', 'nekto', 'notik', 'ticno'], 5: ['DBI', 'aladdinrd', 'bada_dev', 'bdgroup', 'comdi', 'd', 'd-lera', 'd2cio', 'd4m', 'dadget', 'dangry', 'dart', 'darta_systems', 'darudar', 'dashamail', 'dataline', 'datana', 'dataved', 'dauria', 'dbtc', 'dca', 'dcmiran', 'dcoin', 'dct', 'defa', 'defconru', 'deiteriy', 'dell_technologies', 'delphi', 'delta', 'demoscene', 'deskun', 'devicelockdlp', 'devline', 'devmail', 'devprom', 'devtodev', 'diafan', 'digdes', 'digitalangel', 'digitalleague', 'digitalrightscenter', 'dins', 'directum', 'dit', 'django', 'dododev', 'dronk', 'ds', 'dsec', 'dsol', 'dwh', 'gdemoi', 'gdeparking', 'gdeslon', 'grabduck', 'jd', 'joydev', 'landata', 'linux_dev', 'mdk', 'sdn', 'trackduck', 'wd', 'win_dev'], 14: ['Ecology', 'acribia', 'actio', 'alconost', 'arcadia', 'arcserve', 'ardoor', 'arnion', 'ascon', 'astralinux', 'bringo', 'business-laws', 'cdtocenter', 'clodo', 'clojure', 'codabra', 'codeorchestra', 'comsol', 'coronalabs', 'croc', 'io', 'jabra', 'jincor', 'jomportal', 'joom', 'jugru', 'kerio', 'lamoda', 'lawboot', 'ledobzor', 'lenovo', 'lingualeo', 'liteorder', 'lodoss', 'mongodb', 'obeschania', 'octodon', 'ocz', 'oda', 'odindoma', 'ods', 'officerecovery', 'okmeter', 'oktogo', 'opera', 'orbita', 'oriense', 'orienteer', 'orioninc', 'osc', 'osnova', 'osom', 'otus', 'oversun', 'owasp', 'oxygendc', 'ozontech', 'percolator', 'polybius', 'reactos', 'remo', 'sokogroup', 'supereon', 'surfingbird'], 6: ['FastVPS', 'adv', 'advocam', 'amvera', 'antikvariat', 'arvr', 'arvrdev', 'avanpost', 'avlaw', 'bookvario', 'cvs', 'fvdmedia', 'gotech_vc', 'gvalg', 'intervolga', 'lavinatv', 'ovivo_media', 'poiskvps', 'svable', 'truevds', 'ultravds', 'varonis', 'vasexperts', 'vc', 'vcstart', 'vds', 'vdsina', 'veeam', 'venetasystem', 'venturecap', 'vertdider', 'viber', 'vigo', 'virt2real', 'virtualization', 'virus', 'virusdie', 'visa', 'vistlan', 'vivaldi', 'vk', 'vk_api', 'vkarmane', 'vps-menu', 'vps_house', 'vsce', 'vseinstrumenti', 'vsk_insurance', 'vtb', 'vtb_leasing'], 48: ['IPFS', 'aiken', 'cti', 'e-StyleISP', 'gimao', 'giperactive', 'group-ib', 'i2p', 'ibm', 'ibm_bluemix', 'ibs', 'ic-dv', 'icanchoose', 'ichar', 'icl_services', 'icover', 'ics', 'idcee', 'ideaplatform', 'ideco', 'idemand', 'idfinance', 'ifree', 'ifube', 'igromagaz', 'iloveip', 'imody', 'impro', 'inpglobal', 'internet_of_things', 'interneturok', 'intersystems', 'iot_dev', 'ip-concierge', 'iplexgroup', 'iponweb', 'iptelefon', 'iptv', 'iqb_technologies', 'irbis', 'isdef', 'iseo', 'ispmanager', 'ispring', 'istbudget', 'isystems', 'it-guild', 'it-infrastructure', 'iteco', 'itelma', 'itglobalcom', 'iticapital', 'itmai', 'itoolabs', 'itransition', 'itroles', 'itstandarts', 'itsumma', 'ivi', 'izine', 'kauri_iot', 'miip', 'sprint_iidf'], 72: ['JetBrains', 'JetBrains-education', 'brain', 'brainfuck', 'cognitive', 'cognitivepilot', 'geekbrains', 'intminds', 'neuralnet', 'neurodatalab', 'neuronet', 'neuronspace', 'rebrainme'], 66: ['Megasoft', 'acronis', 'adobe', 'aerodisk', 'alloy_software', 'armisoft', 'atompark_software', 'backup', 'carbonsoft', 'cinesoft', 'crystal_service', 'developersoft', 'devrainsolutions', 'driverpack', 'etnasoft', 'galssoftware', 'hardmaster', 'hetmansoftware', 'itsoft', 'litemanager', 'microservices', 'nanosoft', 'nevosoft', 'nodasoft', 'puresoft', 'redsoft', 'regionsoft', 'seagate', 'severstal', 'sigrand', 'simbirsoft', 'smart_soft', 'soft', 'softkey', 'softline', 'softmart', 'softpatent', 'softserve', 'solidity', 'sphere_systems', 'ssp-soft', 'storages', 'sunera', 'symantec', 'synology', 'syssoft', 'terrasoft'], 10: ['Peripheral', 'autogadgets', 'easyelectronics', 'electronics', 'flipperdevices', 'gadgetfreaks', 'gadgets', 'hardware', 'inspectorgadgets', 'medgadgets', 'sberdevices', 'smartgadget', 'vrdevice', 'wearable_electronics', 'yotadevices'], 27: ['SECL_GROUP', 'cobol', 'fortran', 'gaz-is', 'gsgroup', 'matlab', 'ms_access', 'oracle', 'prolog', 'sap', 'sas', 'scada', 'semagroup', 'vba'], 2: ['T1Holding', 'audit-telecom', 'ctcom', 'ctconsult', 'ctf', 'drtariff', 'fgts', 'garstelecom', 'gtd', 'gttf', 'gtv', 'gwt', 'hgst', 'htc', 'htdt', 'kts', 'luckytee', 'mbt', 'mts_ai', 'mtt', 'ntc-vulkan', 'otr', 'otr_to', 'rzltt', 'smart-timetable', 't1consulting', 'taggy', 'talktochef', 'tarantool', 'tayle', 'tceh', 'tdb', 'tdd', 'te_st', 'techtunnel', 'tele2', 'telebreeze', 'telecitygroup', 'telemedicine', 'telhosting', 'telphin', 'telum', 'tensor', 'tensorflow', 'teradata', 'terminator', 'textolite', 'thercon', 'theron', 'tibbo', 'tinkoff', 'tipmeet', 'tmaxsoft', 'tmru', 'tod', 'tomhunter', 'tonk', 'tornado_tech', 'torrentstream', 'tprs', 'travelata', 'triggmine', 'trigl', 'trueconf', 'tssolution', 'tuffle', 'tuthost', 'twisted', 'tzor', 'zte'], 60: ['Techart', 'art', 'arttel', 'canvas', 'creative_commons', 'creativemedia', 'dataart', 'freelance', 'krita', 'pixonic', 'sketchbuilder', 'talentboard', 'talenttech', 'toptechphoto', 'whisperarts'], 19: ['UEFI', 'ua-hosting', 'ubrr', 'uis', 'ultima', 'umatech', 'umkamall', 'uml', 'unet', 'unidata', 'unigine', 'unisender', 'unitsolutions', 'unity', 'universarium', 'uniweb', 'unolabo', 'unova', 'unreal_engine', 'unwds', 'uppza', 'uprock', 'uralsib', 'uteam', 'utex'], 69: ['Voximplant', 'X5Tech', 'Zextras', 'atlex', 'axenix', 'citrix', 'combox', 'comexp', 'comparex', 'crosstech', 'dexp', 'exidetechnologies', 'exness', 'finex', 'haxe', 'hexlet', 'hopox', 'hubex', 'it-lex', 'kinderfox', 'latex', 'lifext', 'linxdatacenter', 'livetex', 'modx', 'moex', 'netfox', 'netwrix', 'nix', 'nixys', 'onlinepbx', 'performix', 'pixli', 'ppbbxx', 'promdex', 'raidix', 'relex', 'sphinx', 'starxoft', 'targetix', 'vertex', 'wirex', 'wix', 'x-com', 'xakep', 'ximad', 'xml', 'xslt', 'zabbix', 'zyxel'], 38: ['abdoc', 'altweb', 'docdoc', 'docplus', 'docsvision', 'doctrine', 'dom24x7', 'domains', 'domclick', 'domrf', 'drweb', 'englishdom', 'freshdoc', 'mistersport', 'realweb', 'spaceweb', 'timeweb', 'ukrnames', 'umbrellaitcom', 'webnames', 'webonmap', 'weborama'], 17: ['accelstor', 'achiever', 'assembler', 'mega_accelerator', 'nerepetitor', 'privacyaccelerator', 'upominator'], 50: ['admarket', 'advantshop', 'all2cart', 'carprice', 'ebaytoday', 'ebayworld', 'hackstore', 'hamstermarketplace', 'likeastore', 'makeshoppro', 'onemarkt', 'priceplan', 'productsense', 'productstar', 'retailrocket', 'sales', 'shopconf', 'shopfans', 'shopozzcom', 'storelab', 'ulmart'], 46: ['agima', 'aori', 'atisu', 'badabu', 'banzai', 'buruki', 'fujitsu', 'gaijin', 'hideninja', 'jitsu', 'kanobu', 'kupikupon', 'meizu', 'mishiko', 'mugenpower', 'rabotaru', 'samurai', 'sipuni', 'sochikamera', 'toshiba', 'toshibarus', 'tuturu', 'uchi_ru', 'zapishis'], 4: ['alfastrah', 'cfc', 'cft', 'f8', 'fabbers', 'fabernovel', 'fairbear', 'fastbduget', 'fastlaneventures', 'fbk_cs', 'fetchee', 'ffcms', 'fido', 'filanco', 'finam', 'fintech_fab', 'firebird', 'firefox', 'fixber', 'flaby', 'flant', 'flapmyport', 'flash_platform', 'flashphoner', 'flask', 'flowwow', 'flprog', 'fondy', 'fortiservice', 'fpga', 'friifond', 'fsf', 'fsharp', 'fsight', 'funcprog', 'futubra', 'hashflare', 'hflabs', 'itfb', 'litemf', 'lsfusion', 'phalcon', 'wunderfund'], 54: ['almeza', 'amnezia', 'avito', 'esprito', 'eviterra', 'exante', 'fixico', 'headzio', 'indexisto', 'inoventica', 'istodo', 'it_dominanta', 'itarena', 'itiviti', 'lenvendo', 'megalenta', 'memiana', 'mercuryo', 'moedelo', 'moemesto', 'perezvoni', 'planado', 'pomodoro', 'portaone', 'potrebiteli', 'pravo', 'prestigio', 'pruffi', 'rizzoma', 'sebbia', 'smestanamesto', 'variti', 'velomesto', 'vexorci', 'virtuozzo', 'vraschete'], 42: ['ashmanov_net', 'balrobotov', 'golovanov_net', 'ivanpr', 'klinika_shilovoy', 'ktovputi', 'ligastavok', 'mkechinov', 'npf_vektor', 'odnoklassniki', 'oleg-bunin', 'ostrovok', 'petrovich', 'pozvonim', 'prodlenka', 'roskomsvoboda', 'shkolnaya_karta', 'skolkovomd', 'sobakapavlova', 'sokatovo', 'sravni', 'svezet', 'svyaznoy', 'vezetvsem', 'vilianov_inc', 'vkrugudruzei', 'vyshtech'], 23: ['astoundcommerce', 'ecommerce', 'ecommerce_development', 'essential_commerce', 'virtocommerce'], 11: ['astronomy', 'bannersabc', 'blackswift', 'caspowa', 'hsespb', 'psb', 'readyforsky', 's7', 'scalaxy', 'sciberia', 'scinmt', 'scione', 'scorocode', 'scout', 'senetsy', 'seopult', 'sezinnopolis', 'skichel', 'skilline', 'skyeng', 'skypro', 'smscoin', 'solarstaff', 'southbridge', 'sp', 'spagece', 'spbifmo', 'spd', 'speereo', 'spottle', 'staply', 'starforce', 'stars_s', 'starwind', 'stc_spb', 'stepic', 'stratoplan', 'stss', 'suplbiz', 'sw', 'zeptobars'], 74: ['audiocodes', 'audiomania', 'sound', 'sound4you', 'soundpal'], 41: ['autodesk', 'desktop_environment', 'desktops', 'helpdesk', 'mytaskhelper', 'notebooks', 'okdesk', 'omnidesk', 'prettytasks', 'service_desk', 'staffcounter', 'teamdesk', 'tracktask'], 25: ['b2broker', 'broker', 'bvc_trade', 'iobroker', 'marketnavigator', 'obrforex', 'otkritie_broker', 'sbermarket', 'sbermegamarket', 'smartmarket', 'textbroker', 'tradingview'], 44: ['bank24', 'bankfilter', 'banki', 'finance', 'gazprombank', 'modulbank', 'raiffeisenbank', 'rosbank', 'sberbank', 'smpbank', 'sovcombank_technologies', 'ubank'], 7: ['bankcreditcard', 'credit_card', 'mastercard'], 43: ['bb-mobile', 'cellular', 'citymobil', 'iridiummobile', 'mobidev', 'mobihunter', 'mobile_dev', 'mobile_dimension', 'mobile_monetization', 'mobileanalytics', 'mobileup', 'nomobile', 'smartphones', 'sonyxperia'], 49: ['beacon-connect', 'cisco', 'cisconetworks', 'cpanetwork', 'dns', 'extremenetworks', 'freelansim', 'ipv6', 'ispsystem', 'itrack', 'mesh_networking', 'netapp', 'netcat', 'netcracker', 'netcube', 'netdev', 'netologyru', 'netrack', 'network_hardware', 'network_standarts', 'network_technologies', 'p2p', 'simnetworks', 'skydns', 'voips', 'wireless', 'wirenboard'], 35: ['beetleplay', 'playgendary', 'playkey', 'playme', 'playrix', 'playtestix', 'playtox'], 70: ['begun', 'findstartup', 'glavstart', 'infostart', 'ingria_startup', 'smart_start', 'starttospeak', 'startupacademy', 'startupindex', 'startuppoint', 'startuprise', 'taptostart', 'wapstart'], 31: ['bimeister', 'binarydistrict', 'bitcalm', 'bitfury', 'bitrix', 'bizone', 'mixbytes', 'sibirix'], 8: ['blackbox', 'boxowerview', 'bright-box', 'embox', 'finobox', 'infobox', 'mindbox', 'onebox', 'pibox', 'skillbox', 'yambox'], 51: ['bm', 'bmstu', 'bmw', 'broadcast', 'camira', 'cbs', 'center2m', 'erstmedia', 'gazprommedia', 'getmobit', 'imobilco', 'infomania', 'itmozg', 'm2m', 'm2tech', 'macroscop', 'magnit', 'maltima', 'mamba', 'mango_telecom', 'marsis', 'masterkit', 'masterup', 'matbea', 'maxifier', 'maxilect', 'maximatelecom', 'mbs', 'media_management', 'mediagates', 'mediagrus', 'mediascope', 'mediatek', 'medica24', 'megafon', 'megaplan', 'merchium', 'metronet', 'metrotek', 'microsoft', 'microsoftlumia', 'milandr', 'mir', 'misis', 'mkb', 'mobilizetoday', 'mobio', 'modesco', 'momondo', 'monandco', 'monosnap', 'moonmodule', 'mosigra', 'motocms', 'motorica', 'moygen', 'msw', 'smartmedia', 'smi2', 'stargetmedia', 'trendmicro', 'zoomtv'], 65: ['byyd', 'gypost', 'yadro', 'yagla', 'yagoodza', 'yandex', 'yandex_api', 'yeaphost', 'yoomoney', 'yota', 'yougile', 'youla', 'youvend', 'zyfra'], 59: ['cakephp', 'cms', 'codeigniter', 'drupal', 'joomla', 'kohanaphp', 'laravel', 'magento', 'php', 'phpshop', 'symfony', 'wordpress', 'yii', 'zend_framework'], 45: ['carrotquest', 'cuiq', 'exeq', 'gfranq', 'goquests', 'monq', 'nqhost', 'prohq', 'qbaka', 'qbs', 'qrator', 'qrobot', 'qsoft', 'qt_software', 'quadcode', 'quantum', 'quarkly', 'quarta', 'qubequ', 'quickme', 'quickresto', 'socialquantum', 'sqalab', 'studyqa'], 40: ['cdnvideo', 'erlyvideo', 'ivideon', 'mvideo', 'teachvideo', 'video', 'video_conferencing', 'video_tech', 'videointellect'], 18: ['club_inno', 'coursmos', 'cronos', 'incos', 'inetra', 'infatica', 'infobip', 'ingos_it', 'innopolis', 'innopolis_university', 'innoros', 'innos', 'innostage', 'innotech', 'innotrio', 'innova', 'inobitec', 'inreco', 'intems', 'invola', 'involta', 'notissimus', 'xronos'], 58: ['cocoa', 'cubecamp', 'ios_dev', 'macbang', 'maccentre', 'macmonitor', 'macster', 'macstreamplatform', 'mcad', 'objectivec', 'osx_dev', 'ru-iphone', 'safari', 'swift', 'xcode'], 29: ['conpay', 'life-pay', 'pacpac', 'payler', 'payrus', 'payture', 'payu'], 57: ['coolrf', 'r', 'r-style', 'r-techno', 'r01', 'r_k', 'radmin', 'rambler', 'rangg', 'raspisanie', 'rcland', 'rcntec', 'rdp', 'rdtex', 'rdv-it', 'readyscript', 'redhelper', 'redkeds', 'redmond', 'redpine', 'redsys', 'regberry', 'reggi', 'regru', 'remote', 'renins', 'ric', 'ricoh', 'riddut', 'rik', 'ringostat', 'ripencc', 'ritm-z', 'rma', 'rnt', 'roem', 'roi4cio', 'roistat', 'rookee', 'rootuamedia', 'rootwelt', 'ror', 'rosatom', 'roseltorg', 'rostelecom', 'rpserver', 'rshb', 'rtit', 'ru_mts', 'rubda', 'runacap', 'rust', 'ruvds', 'ruvpn', 'ruward', 'rvision', 'srg'], 0: ['deutschetelekomitsolutions', 'hofftech'], 68: ['devconf', 'toster', 'toster_conf'], 52: ['e-legion', 'e1tele', 'eaeconsult', 'easla', 'eaton', 'ecm', 'econtenta', 'ecopsy', 'ecwid', 'eczobike', 'edison', 'edusty', 'efo', 'elama', 'elar', 'elbrusbootcamp', 'elm', 'emacs', 'emailmarketing', 'emercoin', 'energoslon', 'engelbart', 'enterra', 'eos', 'epam_systems', 'epson', 'erlang', 'erp', 'eset', 'etagi', 'etegro', 'etmc_exponenta', 'etransport', 'europlan', 'eutelsat', 'evateam', 'evernote', 'everslide', 'evraz', 'evrone', 'exponenta', 'express42', 'ezviz', 'flyelephant', 'lead4crm', 'liveexpertru', 'smile-expo', 'smileexpo'], 9: ['facebook', 'facebook_api', 'flutter', 'infopulse', 'semrush', 'sendpulse', 'social_networks', 'twitter_api', 'websitepulse'], 34: ['fnbk', 'gnivc', 'mnogobyte', 'nag', 'nano', 'nanofl', 'napartner', 'naumen', 'naviaddress', 'navicon', 'nbz', 'nebo', 'neobit', 'netangels', 'nimax', 'ninjamock', 'nlmk', 'nmg', 'nordavind', 'nordclan', 'nornickel', 'notsen', 'novatek', 'npo-comp', 'npoechelon', 'nsd', 'nspk', 'nubes', 'nubex', 'nv_clinic', 'nwlaw', 'pnn', 'snom'], 33: ['google', 'google_api', 'google_chrome', 'google_cloud_vision_api', 'googlecloud', 'osbygoogle'], 61: ['gpgpu', 'videocards'], 67: ['homecredit', 'rencredit', 'saascredit', 'yes-credit'], 73: ['huawei', 'jowi', 'qiwi', 'qiwi_universe', 'wicron'], 24: ['it_testing', 'lamptest', 'testo_lang', 'testutor', 'web_testing'], 71: ['microformats', 'microset'], 32: ['mobile_one', 'mobile_testing'], 26: ['mssql', 'mysql', 'nosql', 'postgrespro', 'postgresql', 'sql', 'sqlite'], 13: ['reksoft']}\n"
     ]
    }
   ],
   "source": [
    "instructor_kmeans = TextClusterization(\n",
    "    INSTRUCTOR(\"hkunlp/instructor-large\"), KMeans(n_clusters=75, random_state=42)\n",
    ")\n",
    "text_instruction_pairs = [\n",
    "    [\n",
    "        \"Represent the Programming Hub category for company or technology or activity clustering:\",\n",
    "        label,\n",
    "    ]\n",
    "    for label in hubs.labels\n",
    "]\n",
    "instructor_kmeans_clusters = instructor_kmeans.get_clusters(text_instruction_pairs)\n",
    "instructor_kmeans_clusters = dict(\n",
    "    (key, [value[1] for value in values])\n",
    "    for key, values in instructor_kmeans_clusters.items()\n",
    ")\n",
    "measure_clusterization(markup, instructor_kmeans_clusters)\n",
    "\n",
    "print(instructor_kmeans_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca6aff",
   "metadata": {},
   "source": [
    "Полученный результат мне не нравится. Думаю проблема - в нехватке контекста, для хороших эмбеддингов нужно больше 1-2 слов.\n",
    "\n",
    "Как нам получить текстовое описание каждого лейбла? Очень просто - давайте выберем из статей с этим лейблом самые частотные (по TF-IDF для него)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d79a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs_top_worlds = hubs.get_top_words_per_label(\"text_markdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57a5f8",
   "metadata": {},
   "source": [
    "Посмотрим примеры, для каких-нибудь не особо понятных слов и для понятных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0944a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intptr', 4065.578369140625),\n",
       " ('param', 623.1882934570312),\n",
       " ('ref', 497.8687438964844),\n",
       " ('uint', 447.219970703125),\n",
       " ('zero', 367.0495300292969),\n",
       " ('oid', 365.9857482910156),\n",
       " ('returns', 345.847412109375),\n",
       " ('summary', 341.7725524902344),\n",
       " ('сертификата', 253.23025512695312),\n",
       " ('подписи', 184.31829833984375)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubs_top_worlds[\"mono\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d850e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('endl', 69.2018051147461),\n",
       " ('nullptr', 67.54036712646484),\n",
       " ('cout', 67.17047882080078),\n",
       " ('typename', 67.06929016113281),\n",
       " ('std', 58.166343688964844),\n",
       " ('size_t', 54.285831451416016),\n",
       " ('gdb', 42.914669036865234),\n",
       " ('clang', 39.875152587890625),\n",
       " ('typedef', 38.802547454833984),\n",
       " ('boost', 37.63223648071289)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubs_top_worlds[\"cpp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08334506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accuracy', 40.30488967895508),\n",
       " ('sklearn', 40.233360290527344),\n",
       " ('predict', 37.253684997558594),\n",
       " ('loss', 34.91127395629883),\n",
       " ('keras', 34.31116485595703),\n",
       " ('ml', 32.905975341796875),\n",
       " ('train', 31.36823272705078),\n",
       " ('plt', 30.495180130004883),\n",
       " ('датасет', 29.038772583007812),\n",
       " ('регрессии', 28.73417854309082)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubs_top_worlds[\"machine_learning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc4135",
   "metadata": {},
   "source": [
    "Выглядит неплохо, может быть это поможет Instructor или mini-LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_learning accuracy sklearn predict loss keras ml train plt датасет регрессии\n"
     ]
    }
   ],
   "source": [
    "def get_text(label, prompt=None, top_words=hubs_top_worlds):\n",
    "    if prompt is None:\n",
    "        return label + \" \" + \" \".join([x for x, _ in top_words[label]])\n",
    "    return [prompt, label + \" \" + \" \".join([x for x, _ in top_words[label]])]\n",
    "\n",
    "\n",
    "print(get_text(\"machine_learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bade64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name hkunlp/instructor-large. Creating a new one with mean pooling.\n",
      "`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'branding'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m instructor_kmeans_clusters = instructor_kmeans.get_clusters(text_instruction_pairs)\n\u001b[32m      7\u001b[39m instructor_kmeans_clusters = \u001b[38;5;28mdict\u001b[39m((key, [value[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values]) \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m instructor_kmeans_clusters.items())\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mmeasure_clusterization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructor_kmeans_clusters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(instructor_kmeans_clusters)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mmeasure_clusterization\u001b[39m\u001b[34m(markup, clusters)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m markup:\n\u001b[32m     11\u001b[39m     l, r, mark = pair\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     l_id, r_id = \u001b[43minverse_clusters\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m, inverse_clusters[r]\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mark == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m l_id == r_id:\n\u001b[32m     15\u001b[39m         TP += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'branding'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "instructor_kmeans = TextClusterization(\n",
    "    INSTRUCTOR(\"hkunlp/instructor-large\"), KMeans(n_clusters=75, random_state=42)\n",
    ")\n",
    "text_instruction_pairs = list(\n",
    "    map(\n",
    "        lambda x: get_text(\n",
    "            x,\n",
    "            \"Represent the Programming Hub category for company or technology or activity clustering: \",\n",
    "        ),\n",
    "        hubs.labels,\n",
    "    )\n",
    ")\n",
    "instructor_kmeans_clusters = instructor_kmeans.get_clusters(text_instruction_pairs)\n",
    "instructor_kmeans_clusters = dict(\n",
    "    (key, [value[1] for value in values])\n",
    "    for key, values in instructor_kmeans_clusters.items()\n",
    ")\n",
    "\n",
    "result = defaultdict(list)\n",
    "for key, values in instructor_kmeans_clusters.items():\n",
    "    for value in values:\n",
    "        label = value.split()[0]\n",
    "        result[key].append(label)\n",
    "\n",
    "measure_clusterization(markup, result)\n",
    "\n",
    "print(instructor_kmeans_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c8fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 1,\n",
       " 'FP': 0,\n",
       " 'FN': 12,\n",
       " 'TN': 30,\n",
       " 'accuracy': 0.7209302325581395,\n",
       " 'false_positives': [],\n",
       " 'false_negatives': [('branding', 'business_models'),\n",
       "  ('business-laws', 'business_models'),\n",
       "  ('research', 'patents'),\n",
       "  ('skyeng', 'learning_languages'),\n",
       "  ('sberbank', 'vtb'),\n",
       "  ('study', 'netologyru'),\n",
       "  ('DIY', 'raspberrypi'),\n",
       "  ('html5', 'javascript'),\n",
       "  ('machine_learning', 'natural_language_processing'),\n",
       "  ('mongodb', 'postgresql'),\n",
       "  ('masterkit', 'medgadgets'),\n",
       "  ('plarium', 'pixonic')]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub.utils import disable_progress_bars\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "disable_progress_bars()\n",
    "\n",
    "minillm_kmeans = TextClusterization(\n",
    "    SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"),\n",
    "    KMeans(n_clusters=50, random_state=42),\n",
    ")\n",
    "\n",
    "minillm_kmeans_clusters = minillm_kmeans.get_clusters(list(map(get_text, hubs.labels)))\n",
    "\n",
    "result_mini_llm = defaultdict(list)\n",
    "for key, values in minillm_kmeans_clusters.items():\n",
    "    for value in values:\n",
    "        label = value.split()[0]\n",
    "        result_mini_llm[key].append(label)\n",
    "\n",
    "measure_clusterization(markup, result_mini_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5a9af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/targets/hubs.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/targets/hubs.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m+x\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      2\u001b[39m     json.dump(result_mini_llm, file, indent=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/habr-article-analyzer/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileExistsError\u001b[39m: [Errno 17] File exists: 'data/targets/hubs.json'"
     ]
    }
   ],
   "source": [
    "with open(\"data/targets/hubs.json\", \"+x\") as file:\n",
    "    json.dump(result_mini_llm, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
