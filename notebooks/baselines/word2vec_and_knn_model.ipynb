{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbec9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from gensim.models import KeyedVectors\n",
    "from habr_article_analyzer.data import load_dataset_from_zst\n",
    "from habr_article_analyzer.data_loader import HabrDataset\n",
    "from habr_article_analyzer.models.baseline.baseline import BaselineWord2VecKNN\n",
    "from habr_article_analyzer.models.encoders.word2vec_encoder import (\n",
    "    BilingualWord2VecEncoder,\n",
    ")\n",
    "from habr_article_analyzer.models.predictors.knn_predictor import KNNPredictor\n",
    "from habr_article_analyzer.settings import data_settings, settings\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200d273",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "*Author: Nikita Zolin*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c262e",
   "metadata": {},
   "source": [
    "The goal of this notebook is to prepare the baseline. Our task: predict the probability of each hub for the given text. Let's explain how the model will work:\n",
    "\n",
    "1. We take two inputs: text and hub, which we need to map to some vectors. Model `A` will map text to some vector in $R^n$, model `B` will map hub to some vector in $R^m$.\n",
    "2. We concatenate these vectors to get one vector in $R^{n+m}$.\n",
    "3. Model `C` estimates the probability based on this vector.\n",
    "\n",
    "In this notebook we will use word2vec for models `A` and `B` and we will adjust KNN for model `C`.\n",
    "\n",
    "However, before fitting the model we need to gather the dataset. Here, as it's just a baseline, we will simply take one positive and three random negative hubs for each text. This logic is implemented in [data_utils](../../src/habr_article_analyzer/data_utils/) and was run as a module before the code below.\n",
    "\n",
    "Then, we need to install some pretrained word2vec. Let's download them: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f221d-1d93-4435-830d-586565043915",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "\n",
    "We will take the small ones to run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f8e4ee-3025-4c4a-a414-90ee6dd62c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_en = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137c49d2-2450-40f8-b3f3-96c8bbe0a851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kv_ru = api.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682fa59-da66-45a9-9a5c-10d384518b1c",
   "metadata": {},
   "source": [
    "## Model hyperparamateres optimization\n",
    "\n",
    "Let's fix some metric, for example `ROC AUC`, and will find the best hyperparameter for the model. There we will run on the small sizes due to limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b78d05-367b-475b-bdf9-1cd06e0243a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading dataset: 1751498it [01:27, 20086.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare mini-dataset for local run\n",
    "np.random.seed(data_settings.random_seed)\n",
    "\n",
    "dataset = HabrDataset(\n",
    "    path=settings.raw_data_dir / \"train_with_negatives.jsonl.zst\",\n",
    "    columns=[\"text\", \"hub\", \"label\"],\n",
    "    batch_size=data_settings.batch_size,\n",
    ")\n",
    "\n",
    "selected_rows = []\n",
    "for batch_df in dataset:\n",
    "    mask = np.random.rand(len(batch_df)) < 0.005  # Select 0.5% mask\n",
    "    selected_rows.append(batch_df[mask])\n",
    "\n",
    "train_df_sample = pd.concat(selected_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ef2a3f-68ce-451f-b501-e4509b99303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8781, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76f6ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85e5b85befb4f8297d430e87c244840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n_neighbors:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14ffbe776f24ae5bb8e234bb4e86988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce921636e13a4ed5895a3052dd730145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3ae2f9f941409db715e4f03013e793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " np.float64(0.7247423154966111),\n",
       " {3: np.float64(0.7062915611986679),\n",
       "  5: np.float64(0.7211842984326772),\n",
       "  7: np.float64(0.7247423154966111)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for n_neighbors\n",
    "encoder = BilingualWord2VecEncoder(kv_ru=kv_ru, kv_en=kv_en)\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=data_settings.random_seed)\n",
    "\n",
    "texts = train_df_sample[\"text\"]\n",
    "hubs = train_df_sample[\"hub\"]\n",
    "labels = train_df_sample[\"label\"]\n",
    "\n",
    "result: dict[int, float] = {}\n",
    "\n",
    "for n_neighbors in tqdm([3, 5, 7], desc=\"n_neighbors\"):\n",
    "    knn = KNNPredictor(n_neighbors=n_neighbors)\n",
    "\n",
    "    model = BaselineWord2VecKNN(\n",
    "        text_encoder=encoder, hub_encoder=encoder, predictor=knn\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tqdm(\n",
    "        kfold.split(texts, labels), total=kfold.n_splits, desc=f\"folds\", leave=False\n",
    "    ):\n",
    "        # split\n",
    "        X_train_texts = texts[train_idx]\n",
    "        X_train_hubs = hubs[train_idx]\n",
    "        y_train = labels[train_idx]\n",
    "\n",
    "        X_val_texts = texts[val_idx]\n",
    "        X_val_hubs = hubs[val_idx]\n",
    "        y_val = labels[val_idx]\n",
    "\n",
    "        # train\n",
    "        model.fit(X_train_texts, X_train_hubs, y_train)\n",
    "\n",
    "        # predict proba\n",
    "        probas = [\n",
    "            model.predict_proba(text, hub) for text, hub in zip(X_val_texts, X_val_hubs)\n",
    "        ]\n",
    "\n",
    "        # score\n",
    "        score = roc_auc_score(y_val, probas)\n",
    "        scores.append(score)\n",
    "\n",
    "    result[n_neighbors] = np.array(scores).mean()\n",
    "\n",
    "best_key = max(result, key=result.get)\n",
    "best_value = result[best_key]\n",
    "\n",
    "best_key, best_value, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6e8e1",
   "metadata": {},
   "source": [
    "So now let's stick to this parameter and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a7132d-9115-4f3b-8b7a-67bf71b3be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNPredictor(n_neighbors=best_key)\n",
    "model = BaselineWord2VecKNN(text_encoder=encoder, hub_encoder=encoder, predictor=knn)\n",
    "\n",
    "model.fit(train_df_sample[\"text\"], train_df_sample[\"hub\"], train_df_sample[\"label\"])\n",
    "\n",
    "model.save(settings.models_dir / \"baseline_word2vec_knn.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7574b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading records: 437367it [00:24, 18155.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.7127623316361188),\n",
       " (1, 0.7140813997294929),\n",
       " (1, 0.7127623316361188),\n",
       " (1, 0.7127623316361188),\n",
       " (0, 0.7127623316361188),\n",
       " (0, 0.7127623316361188),\n",
       " (0, 0.7127623316361188),\n",
       " (0, 0.5625827767958712),\n",
       " (0, 0.0),\n",
       " (1, 0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_dataset_from_zst(settings.raw_data_dir / \"test_with_negatives.jsonl.zst\")\n",
    "\n",
    "# Decrease the test sample to run it locally\n",
    "test_texts = test_df[\"text\"].tolist()[:1000]\n",
    "test_hubs = test_df[\"hub\"].tolist()[:1000]\n",
    "test_labels = test_df[\"label\"].tolist()[:1000]\n",
    "\n",
    "\n",
    "probas = [model.predict_proba(text, hub) for text, hub in zip(test_texts, test_hubs)]\n",
    "\n",
    "list(zip(test_labels[:10], probas[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f437f74-7056-48e6-8f05-5f0547b46a87",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Let's use some standard metric to estimate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ffe1ec-b962-4584-83ba-64b0b67578b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7311\n",
      "Log Loss: 1.7679\n"
     ]
    }
   ],
   "source": [
    "probas = np.array(probas)\n",
    "labels = np.array(test_labels)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(labels, probas)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Log Loss\n",
    "ll = log_loss(labels, probas)\n",
    "print(f\"Log Loss: {ll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1eb28-d587-4137-aff3-f0cd49bb88c8",
   "metadata": {},
   "source": [
    "On this step it's hard to say if the results are good because it's our first model, but now we have something and are able to compare our future models to these metrics. However, our main goal is to use one specific metric to compare different models, which will be presented in a different notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
